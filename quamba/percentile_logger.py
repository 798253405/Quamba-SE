"""
Percentile Logger - è®°å½•percentileå’Œreorderå¯¹æ¿€æ´»èŒƒå›´çš„å½±å“
"""

import json
import logging
from datetime import datetime
from pathlib import Path
import sys
import numpy as np

class PercentileLogger:
    """è®°å½•percentileå®éªŒçš„è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯"""

    def __init__(self, log_file="percentile_experiments.jsonl", save_activations=False):
        self.log_file = Path(log_file)
        self.save_activations = save_activations
        self.activation_samples = {}  # ç”¨äºä¿å­˜æ¿€æ´»å€¼æ ·æœ¬

        self.current_experiment = {
            "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "command": " ".join(sys.argv),
            "config": {},
            "activation_stats": {},
            "reorder_summary": {
                "enabled": False,
                "avg_range_reduction": None,
                "total_layers": 0
            },
            "results": {},
            "activation_data_file": None  # å¦‚æœä¿å­˜äº†æ¿€æ´»å€¼ï¼Œè®°å½•æ–‡ä»¶è·¯å¾„
        }

    def log_config(self, args):
        """è®°å½•å®éªŒé…ç½®"""
        self.current_experiment["config"] = {
            "model": args.model,
            "w_bits": args.w_bits,
            "a_bits": args.a_bits,
            "percentile_alpha": args.percentile_alpha,
            "group_heads": args.group_heads,
            "apply_gptq": args.apply_gptq,
            "quantize_embedding": args.quantize_embedding,
            "quantize_lm_head": args.quantize_lm_head,
            "calib_data_num": args.calib_data_num,
            "calib_seqlen": args.calib_seqlen
        }

    def log_activation_stats(self, layer_name, stats, activation_values=None):
        """
        è®°å½•æ¿€æ´»ç»Ÿè®¡ä¿¡æ¯

        Args:
            layer_name: å±‚åç§°
            stats: åŒ…å«ä»¥ä¸‹å­—æ®µçš„å­—å…¸
                - before_percentile: {"min", "max", "range"}
                - after_percentile: {"min", "max", "range"}
                - percentile_alpha: float
                - clipped_ratio: float (è¢«è£å‰ªçš„æ¯”ä¾‹)
            activation_values: (å¯é€‰) å®é™…çš„æ¿€æ´»å€¼ï¼Œç”¨äºå¤ç°
        """
        self.current_experiment["activation_stats"][layer_name] = stats

        # å¦‚æœéœ€è¦ä¿å­˜æ¿€æ´»å€¼
        if self.save_activations and activation_values is not None:
            # åªä¿å­˜å‰100ä¸ªæ ·æœ¬ï¼Œé¿å…æ–‡ä»¶è¿‡å¤§
            if isinstance(activation_values, np.ndarray):
                self.activation_samples[layer_name] = activation_values.flatten()[:10000].tolist()
            else:
                # PyTorch tensor
                self.activation_samples[layer_name] = activation_values.flatten()[:10000].cpu().numpy().tolist()

    def log_reorder_impact(self, layer_name, before_reorder, after_reorder):
        """
        è®°å½•reorderå¯¹æ¿€æ´»èŒƒå›´çš„å½±å“

        Args:
            layer_name: å±‚åç§°
            before_reorder: {"min", "max", "range"}
            after_reorder: {"min", "max", "range"}
        """
        if layer_name in self.current_experiment["activation_stats"]:
            self.current_experiment["activation_stats"][layer_name]["before_reorder"] = before_reorder
            self.current_experiment["activation_stats"][layer_name]["after_reorder"] = after_reorder
            self.current_experiment["activation_stats"][layer_name]["range_reduction_pct"] = (
                (before_reorder["range"] - after_reorder["range"]) / before_reorder["range"] * 100
            )

        self.current_experiment["reorder_summary"]["enabled"] = True

    def compute_reorder_summary(self):
        """è®¡ç®—reorderçš„æ€»ä½“æ•ˆæœ"""
        if not self.current_experiment["reorder_summary"]["enabled"]:
            return

        reductions = []
        total_layers = 0

        for layer_name, stats in self.current_experiment["activation_stats"].items():
            if "range_reduction_pct" in stats:
                reductions.append(stats["range_reduction_pct"])
                total_layers += 1

        if reductions:
            self.current_experiment["reorder_summary"]["avg_range_reduction"] = sum(reductions) / len(reductions)
            self.current_experiment["reorder_summary"]["total_layers"] = total_layers

    def log_results(self, accuracy, perplexity):
        """è®°å½•æœ€ç»ˆç»“æœ"""
        self.current_experiment["results"] = {
            "accuracy": float(accuracy),
            "perplexity": float(perplexity)
        }

    def save(self):
        """ä¿å­˜åˆ°JSONLæ–‡ä»¶ï¼ˆè¿½åŠ æ¨¡å¼ï¼‰"""
        self.compute_reorder_summary()

        # åˆ›å»ºç›®å½•
        self.log_file.parent.mkdir(parents=True, exist_ok=True)

        # å¦‚æœæœ‰æ¿€æ´»å€¼æ ·æœ¬ï¼Œä¿å­˜åˆ°å•ç‹¬çš„.npzæ–‡ä»¶
        if self.activation_samples:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            model_name = self.current_experiment["config"]["model"].split('/')[-1]
            pa = self.current_experiment["config"].get("percentile_alpha", "default")
            pa_str = f"pa{pa}" if pa != "default" else "default"

            activation_file = self.log_file.parent / f"activations_{model_name}_{pa_str}_{timestamp}.npz"
            np.savez_compressed(activation_file, **self.activation_samples)
            self.current_experiment["activation_data_file"] = str(activation_file)
            logging.info(f"âœ… æ¿€æ´»å€¼å·²ä¿å­˜åˆ°: {activation_file}")

        # è¿½åŠ å†™å…¥JSONæ—¥å¿—
        with open(self.log_file, 'a') as f:
            f.write(json.dumps(self.current_experiment, indent=None) + "\n")

        logging.info(f"âœ… Percentileå®éªŒæ—¥å¿—å·²ä¿å­˜åˆ°: {self.log_file}")

    def print_summary(self):
        """æ‰“å°å½“å‰å®éªŒçš„æ‘˜è¦"""
        print("\n" + "="*80)
        print("ğŸ“Š Percentileå®éªŒæ‘˜è¦")
        print("="*80)

        # é…ç½®ä¿¡æ¯
        config = self.current_experiment["config"]
        print(f"\nğŸ”§ é…ç½®:")
        print(f"  æ¨¡å‹: {config['model']}")
        print(f"  é‡åŒ–: W{config['w_bits']}A{config['a_bits']}")
        print(f"  Percentile Alpha: {config['percentile_alpha']}")
        print(f"  Group Heads: {config['group_heads']}")

        # æ¿€æ´»ç»Ÿè®¡æ‘˜è¦
        if self.current_experiment["activation_stats"]:
            print(f"\nğŸ“ˆ æ¿€æ´»ç»Ÿè®¡ (å…±{len(self.current_experiment['activation_stats'])}å±‚):")

            # è®¡ç®—å¹³å‡å€¼
            avg_range_before = 0
            avg_range_after = 0
            avg_clip_ratio = 0
            count = 0

            for layer_name, stats in list(self.current_experiment["activation_stats"].items())[:3]:
                if "before_percentile" in stats:
                    print(f"\n  {layer_name}:")
                    print(f"    Percentileè£å‰ªå‰: [{stats['before_percentile']['min']:.2f}, {stats['before_percentile']['max']:.2f}] "
                          f"èŒƒå›´={stats['before_percentile']['range']:.2f}")
                    print(f"    Percentileè£å‰ªå: [{stats['after_percentile']['min']:.2f}, {stats['after_percentile']['max']:.2f}] "
                          f"èŒƒå›´={stats['after_percentile']['range']:.2f}")

                    if "clipped_ratio" in stats:
                        print(f"    è¢«è£å‰ªæ¯”ä¾‹: {stats['clipped_ratio']*100:.4f}%")

                    avg_range_before += stats['before_percentile']['range']
                    avg_range_after += stats['after_percentile']['range']
                    if "clipped_ratio" in stats:
                        avg_clip_ratio += stats['clipped_ratio']
                    count += 1

            if count > 0:
                print(f"\n  å‰3å±‚å¹³å‡:")
                print(f"    è£å‰ªå‰å¹³å‡èŒƒå›´: {avg_range_before/count:.2f}")
                print(f"    è£å‰ªåå¹³å‡èŒƒå›´: {avg_range_after/count:.2f}")
                print(f"    èŒƒå›´ç¼©å°: {(avg_range_before-avg_range_after)/avg_range_before*100:.2f}%")
                if avg_clip_ratio > 0:
                    print(f"    å¹³å‡è£å‰ªæ¯”ä¾‹: {avg_clip_ratio/count*100:.4f}%")

        # Reorderæ•ˆæœ
        reorder = self.current_experiment["reorder_summary"]
        if reorder["enabled"]:
            print(f"\nğŸ”„ Reorderæ•ˆæœ:")
            print(f"  å½±å“å±‚æ•°: {reorder['total_layers']}")
            if reorder["avg_range_reduction"] is not None:
                print(f"  å¹³å‡èŒƒå›´ç¼©å°: {reorder['avg_range_reduction']:.2f}%")

        # æœ€ç»ˆç»“æœ
        if self.current_experiment["results"]:
            results = self.current_experiment["results"]
            print(f"\nğŸ¯ æœ€ç»ˆç»“æœ:")
            print(f"  Accuracy: {results['accuracy']*100:.2f}%")
            print(f"  Perplexity: {results['perplexity']:.3f}")

        print("\n" + "="*80 + "\n")


# å…¨å±€å•ä¾‹
_global_logger = None

def get_percentile_logger(log_file="percentileRangeResults/experiments.jsonl", save_activations=False):
    """è·å–å…¨å±€percentile logger"""
    global _global_logger
    if _global_logger is None:
        _global_logger = PercentileLogger(log_file, save_activations=save_activations)
    return _global_logger

def reset_percentile_logger(log_file="percentileRangeResults/experiments.jsonl", save_activations=False):
    """é‡ç½®å…¨å±€loggerï¼ˆç”¨äºæ–°å®éªŒï¼‰"""
    global _global_logger
    _global_logger = PercentileLogger(log_file, save_activations=save_activations)
    return _global_logger
