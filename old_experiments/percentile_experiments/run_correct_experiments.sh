#!/bin/bash

# Quamba æ­£ç¡®å®éªŒè„šæœ¬ - æ ¹æ®ä½œè€…å›å¤ä¿®æ­£
# Quamba1: Mamba1 W8A8 (ä¸åŠ  embedding/lm_head/gptq)
# Quamba2: Mamba2 W4A8/W4A16/W8A8 (åŠ æ‰€æœ‰å‚æ•°)
# æ¯ä¸ªé…ç½®åš default + pa=1.0 å¯¹æ¯”
# æ€»å…± 20 ä¸ªå®éªŒ
# ä½œè€…ï¼šYZ
# æ—¥æœŸï¼š2025-11-04

set -e  # é‡åˆ°é”™è¯¯ç«‹å³åœæ­¢

LOG_FILE="run_correct_experiments_$(date +%Y%m%d_%H%M%S).log"
exec 2>&1 | tee -a "$LOG_FILE"

echo "========================================================================"
echo "ğŸš€ å¼€å§‹è¿è¡Œæ­£ç¡®çš„ Quamba1 + Quamba2 å®éªŒ"
echo "========================================================================"
echo "Quamba1: Mamba1 W8A8 (æ— é¢å¤–å‚æ•°) - 8ä¸ªå®éªŒ"
echo "Quamba2: Mamba2 W4/W8 (æœ‰é¢å¤–å‚æ•°) - 12ä¸ªå®éªŒ"
echo "æ€»å®éªŒæ•°: 20 ä¸ª"
echo "é¢„è®¡æ€»æ—¶é—´: 6-8 å°æ—¶"
echo "å¼€å§‹æ—¶é—´: $(date)"
echo "æ—¥å¿—æ–‡ä»¶: $LOG_FILE"
echo ""

# ============================================================================
# Quamba1 éƒ¨åˆ†ï¼šMamba1 W8A8 (ä¸åŠ  embedding/lm_head/gptq)
# ä¿å­˜è·¯å¾„ï¼špretrained_models/quamba1/default/ å’Œ quamba1/pa-1/
# ============================================================================

echo "========================================================================"
echo "ğŸ”µ Quamba1 å®éªŒå¼€å§‹"
echo "========================================================================"
echo ""

echo "========================================================================"
echo "ğŸ“Š Quamba1: Mamba1 130M W8A8 - é»˜è®¤ (1/20)"
echo "========================================================================"
echo "å¼€å§‹æ—¶é—´: $(date)"
python3 main.py pretrained_models/mambaOriginalHuggingfaceDownload/mamba-130m \
  --quantize \
  --w_bits 8 \
  --a_bits 8 \
  --batch_size 16 \
  --eval_zero_shot \
  --task_list lambada_openai \
  --pretrained_dir ./pretrained_models \
  --log_dir logs \
  --output_subdir quamba1
echo "âœ… å®Œæˆæ—¶é—´: $(date)"
echo ""

echo "========================================================================"
echo "ğŸ“Š Quamba1: Mamba1 130M W8A8 - pa=1.0 (2/20)"
echo "========================================================================"
echo "å¼€å§‹æ—¶é—´: $(date)"
python3 main.py pretrained_models/mambaOriginalHuggingfaceDownload/mamba-130m \
  --quantize \
  --w_bits 8 \
  --a_bits 8 \
  --percentile_alpha 1.0 \
  --batch_size 16 \
  --eval_zero_shot \
  --task_list lambada_openai \
  --pretrained_dir ./pretrained_models \
  --log_dir logs \
  --output_subdir quamba1
echo "âœ… å®Œæˆæ—¶é—´: $(date)"
echo ""

echo "========================================================================"
echo "ğŸ“Š Quamba1: Mamba1 370M W8A8 - é»˜è®¤ (3/20)"
echo "========================================================================"
echo "å¼€å§‹æ—¶é—´: $(date)"
python3 main.py pretrained_models/mambaOriginalHuggingfaceDownload/mamba-370m \
  --quantize \
  --w_bits 8 \
  --a_bits 8 \
  --batch_size 16 \
  --eval_zero_shot \
  --task_list lambada_openai \
  --pretrained_dir ./pretrained_models \
  --log_dir logs \
  --output_subdir quamba1
echo "âœ… å®Œæˆæ—¶é—´: $(date)"
echo ""

echo "========================================================================"
echo "ğŸ“Š Quamba1: Mamba1 370M W8A8 - pa=1.0 (4/20)"
echo "========================================================================"
echo "å¼€å§‹æ—¶é—´: $(date)"
python3 main.py pretrained_models/mambaOriginalHuggingfaceDownload/mamba-370m \
  --quantize \
  --w_bits 8 \
  --a_bits 8 \
  --percentile_alpha 1.0 \
  --batch_size 16 \
  --eval_zero_shot \
  --task_list lambada_openai \
  --pretrained_dir ./pretrained_models \
  --log_dir logs \
  --output_subdir quamba1
echo "âœ… å®Œæˆæ—¶é—´: $(date)"
echo ""

echo "========================================================================"
echo "ğŸ“Š Quamba1: Mamba1 1.4B W8A8 - é»˜è®¤ (5/20)"
echo "========================================================================"
echo "å¼€å§‹æ—¶é—´: $(date)"
python3 main.py pretrained_models/mambaOriginalHuggingfaceDownload/mamba-1.4b \
  --quantize \
  --w_bits 8 \
  --a_bits 8 \
  --batch_size 16 \
  --eval_zero_shot \
  --task_list lambada_openai \
  --pretrained_dir ./pretrained_models \
  --log_dir logs \
  --output_subdir quamba1
echo "âœ… å®Œæˆæ—¶é—´: $(date)"
echo ""

echo "========================================================================"
echo "ğŸ“Š Quamba1: Mamba1 1.4B W8A8 - pa=1.0 (6/20)"
echo "========================================================================"
echo "å¼€å§‹æ—¶é—´: $(date)"
python3 main.py pretrained_models/mambaOriginalHuggingfaceDownload/mamba-1.4b \
  --quantize \
  --w_bits 8 \
  --a_bits 8 \
  --percentile_alpha 1.0 \
  --batch_size 16 \
  --eval_zero_shot \
  --task_list lambada_openai \
  --pretrained_dir ./pretrained_models \
  --log_dir logs \
  --output_subdir quamba1
echo "âœ… å®Œæˆæ—¶é—´: $(date)"
echo ""

echo "========================================================================"
echo "ğŸ“Š Quamba1: Mamba1 2.8B W8A8 - é»˜è®¤ (7/20)"
echo "========================================================================"
echo "å¼€å§‹æ—¶é—´: $(date)"
python3 main.py pretrained_models/mambaOriginalHuggingfaceDownload/mamba-2.8b \
  --quantize \
  --w_bits 8 \
  --a_bits 8 \
  --batch_size 16 \
  --eval_zero_shot \
  --task_list lambada_openai \
  --pretrained_dir ./pretrained_models \
  --log_dir logs \
  --output_subdir quamba1
echo "âœ… å®Œæˆæ—¶é—´: $(date)"
echo ""

echo "========================================================================"
echo "ğŸ“Š Quamba1: Mamba1 2.8B W8A8 - pa=1.0 (8/20)"
echo "========================================================================"
echo "å¼€å§‹æ—¶é—´: $(date)"
python3 main.py pretrained_models/mambaOriginalHuggingfaceDownload/mamba-2.8b \
  --quantize \
  --w_bits 8 \
  --a_bits 8 \
  --percentile_alpha 1.0 \
  --batch_size 16 \
  --eval_zero_shot \
  --task_list lambada_openai \
  --pretrained_dir ./pretrained_models \
  --log_dir logs \
  --output_subdir quamba1
echo "âœ… å®Œæˆæ—¶é—´: $(date)"
echo ""

# ============================================================================
# Quamba2 éƒ¨åˆ†ï¼šMamba2 W4A8/W4A16/W8A8 (åŠ  group_heads/gptq/embedding/lm_head)
# ä¿å­˜è·¯å¾„ï¼špretrained_models/quamba2/default/ å’Œ quamba2/pa-1/
# ============================================================================

echo "========================================================================"
echo "ğŸŸ¢ Quamba2 å®éªŒå¼€å§‹"
echo "========================================================================"
echo ""

# ---------- Mamba2 2.7B ----------

echo "========================================================================"
echo "ğŸ“Š Quamba2: Mamba2 2.7B W4A8 - é»˜è®¤ (9/20)"
echo "========================================================================"
echo "å¼€å§‹æ—¶é—´: $(date)"
python3 main.py state-spaces/mamba2-2.7b \
  --quantize \
  --group_heads \
  --apply_gptq \
  --quantize_embedding \
  --quantize_lm_head \
  --w_bits 4 \
  --a_bits 8 \
  --batch_size 16 \
  --eval_zero_shot \
  --task_list lambada_openai \
  --pretrained_dir ./pretrained_models \
  --log_dir logs \
  --output_subdir quamba2
echo "âœ… å®Œæˆæ—¶é—´: $(date)"
echo ""

echo "========================================================================"
echo "ğŸ“Š Quamba2: Mamba2 2.7B W4A8 - pa=1.0 (10/20)"
echo "========================================================================"
echo "å¼€å§‹æ—¶é—´: $(date)"
python3 main.py state-spaces/mamba2-2.7b \
  --quantize \
  --group_heads \
  --apply_gptq \
  --quantize_embedding \
  --quantize_lm_head \
  --w_bits 4 \
  --a_bits 8 \
  --percentile_alpha 1.0 \
  --batch_size 16 \
  --eval_zero_shot \
  --task_list lambada_openai \
  --pretrained_dir ./pretrained_models \
  --log_dir logs \
  --output_subdir quamba2
echo "âœ… å®Œæˆæ—¶é—´: $(date)"
echo ""

echo "========================================================================"
echo "ğŸ“Š Quamba2: Mamba2 2.7B W4A16 - é»˜è®¤ (11/20)"
echo "========================================================================"
echo "å¼€å§‹æ—¶é—´: $(date)"
python3 main.py state-spaces/mamba2-2.7b \
  --quantize \
  --group_heads \
  --apply_gptq \
  --quantize_embedding \
  --quantize_lm_head \
  --w_bits 4 \
  --a_bits 16 \
  --batch_size 16 \
  --eval_zero_shot \
  --task_list lambada_openai \
  --pretrained_dir ./pretrained_models \
  --log_dir logs \
  --output_subdir quamba2
echo "âœ… å®Œæˆæ—¶é—´: $(date)"
echo ""

echo "========================================================================"
echo "ğŸ“Š Quamba2: Mamba2 2.7B W4A16 - pa=1.0 (12/20)"
echo "========================================================================"
echo "å¼€å§‹æ—¶é—´: $(date)"
python3 main.py state-spaces/mamba2-2.7b \
  --quantize \
  --group_heads \
  --apply_gptq \
  --quantize_embedding \
  --quantize_lm_head \
  --w_bits 4 \
  --a_bits 16 \
  --percentile_alpha 1.0 \
  --batch_size 16 \
  --eval_zero_shot \
  --task_list lambada_openai \
  --pretrained_dir ./pretrained_models \
  --log_dir logs \
  --output_subdir quamba2
echo "âœ… å®Œæˆæ—¶é—´: $(date)"
echo ""

echo "========================================================================"
echo "ğŸ“Š Quamba2: Mamba2 2.7B W8A8 - é»˜è®¤ (13/20)"
echo "========================================================================"
echo "å¼€å§‹æ—¶é—´: $(date)"
python3 main.py state-spaces/mamba2-2.7b \
  --quantize \
  --group_heads \
  --apply_gptq \
  --quantize_embedding \
  --quantize_lm_head \
  --w_bits 8 \
  --a_bits 8 \
  --batch_size 16 \
  --eval_zero_shot \
  --task_list lambada_openai \
  --pretrained_dir ./pretrained_models \
  --log_dir logs \
  --output_subdir quamba2
echo "âœ… å®Œæˆæ—¶é—´: $(date)"
echo ""

echo "========================================================================"
echo "ğŸ“Š Quamba2: Mamba2 2.7B W8A8 - pa=1.0 (14/20)"
echo "========================================================================"
echo "å¼€å§‹æ—¶é—´: $(date)"
python3 main.py state-spaces/mamba2-2.7b \
  --quantize \
  --group_heads \
  --apply_gptq \
  --quantize_embedding \
  --quantize_lm_head \
  --w_bits 8 \
  --a_bits 8 \
  --percentile_alpha 1.0 \
  --batch_size 16 \
  --eval_zero_shot \
  --task_list lambada_openai \
  --pretrained_dir ./pretrained_models \
  --log_dir logs \
  --output_subdir quamba2
echo "âœ… å®Œæˆæ—¶é—´: $(date)"
echo ""

# ---------- Mamba2 8B ----------

echo "========================================================================"
echo "ğŸ“Š Quamba2: Mamba2 8B W4A8 - é»˜è®¤ (15/20)"
echo "========================================================================"
echo "å¼€å§‹æ—¶é—´: $(date)"
python3 main.py pretrained_models/mambaOriginalHuggingfaceDownload/mamba2-8b-converted \
  --quantize \
  --group_heads \
  --apply_gptq \
  --quantize_embedding \
  --quantize_lm_head \
  --w_bits 4 \
  --a_bits 8 \
  --batch_size 16 \
  --eval_zero_shot \
  --task_list lambada_openai \
  --pretrained_dir ./pretrained_models \
  --log_dir logs \
  --output_subdir quamba2
echo "âœ… å®Œæˆæ—¶é—´: $(date)"
echo ""

echo "========================================================================"
echo "ğŸ“Š Quamba2: Mamba2 8B W4A8 - pa=1.0 (16/20)"
echo "========================================================================"
echo "å¼€å§‹æ—¶é—´: $(date)"
python3 main.py pretrained_models/mambaOriginalHuggingfaceDownload/mamba2-8b-converted \
  --quantize \
  --group_heads \
  --apply_gptq \
  --quantize_embedding \
  --quantize_lm_head \
  --w_bits 4 \
  --a_bits 8 \
  --percentile_alpha 1.0 \
  --batch_size 16 \
  --eval_zero_shot \
  --task_list lambada_openai \
  --pretrained_dir ./pretrained_models \
  --log_dir logs \
  --output_subdir quamba2
echo "âœ… å®Œæˆæ—¶é—´: $(date)"
echo ""

echo "========================================================================"
echo "ğŸ“Š Quamba2: Mamba2 8B W4A16 - é»˜è®¤ (17/20)"
echo "========================================================================"
echo "å¼€å§‹æ—¶é—´: $(date)"
python3 main.py pretrained_models/mambaOriginalHuggingfaceDownload/mamba2-8b-converted \
  --quantize \
  --group_heads \
  --apply_gptq \
  --quantize_embedding \
  --quantize_lm_head \
  --w_bits 4 \
  --a_bits 16 \
  --batch_size 16 \
  --eval_zero_shot \
  --task_list lambada_openai \
  --pretrained_dir ./pretrained_models \
  --log_dir logs \
  --output_subdir quamba2
echo "âœ… å®Œæˆæ—¶é—´: $(date)"
echo ""

echo "========================================================================"
echo "ğŸ“Š Quamba2: Mamba2 8B W4A16 - pa=1.0 (18/20)"
echo "========================================================================"
echo "å¼€å§‹æ—¶é—´: $(date)"
python3 main.py pretrained_models/mambaOriginalHuggingfaceDownload/mamba2-8b-converted \
  --quantize \
  --group_heads \
  --apply_gptq \
  --quantize_embedding \
  --quantize_lm_head \
  --w_bits 4 \
  --a_bits 16 \
  --percentile_alpha 1.0 \
  --batch_size 16 \
  --eval_zero_shot \
  --task_list lambada_openai \
  --pretrained_dir ./pretrained_models \
  --log_dir logs \
  --output_subdir quamba2
echo "âœ… å®Œæˆæ—¶é—´: $(date)"
echo ""

echo "========================================================================"
echo "ğŸ“Š Quamba2: Mamba2 8B W8A8 - é»˜è®¤ (19/20)"
echo "========================================================================"
echo "å¼€å§‹æ—¶é—´: $(date)"
python3 main.py pretrained_models/mambaOriginalHuggingfaceDownload/mamba2-8b-converted \
  --quantize \
  --group_heads \
  --apply_gptq \
  --quantize_embedding \
  --quantize_lm_head \
  --w_bits 8 \
  --a_bits 8 \
  --batch_size 16 \
  --eval_zero_shot \
  --task_list lambada_openai \
  --pretrained_dir ./pretrained_models \
  --log_dir logs \
  --output_subdir quamba2
echo "âœ… å®Œæˆæ—¶é—´: $(date)"
echo ""

echo "========================================================================"
echo "ğŸ“Š Quamba2: Mamba2 8B W8A8 - pa=1.0 (20/20)"
echo "========================================================================"
echo "å¼€å§‹æ—¶é—´: $(date)"
python3 main.py pretrained_models/mambaOriginalHuggingfaceDownload/mamba2-8b-converted \
  --quantize \
  --group_heads \
  --apply_gptq \
  --quantize_embedding \
  --quantize_lm_head \
  --w_bits 8 \
  --a_bits 8 \
  --percentile_alpha 1.0 \
  --batch_size 16 \
  --eval_zero_shot \
  --task_list lambada_openai \
  --pretrained_dir ./pretrained_models \
  --log_dir logs \
  --output_subdir quamba2
echo "âœ… å®Œæˆæ—¶é—´: $(date)"
echo ""

# ============================================================================
# å®Œæˆ
# ============================================================================

echo "========================================================================"
echo "ğŸ‰ æ‰€æœ‰ 20 ä¸ªå®éªŒå®Œæˆï¼"
echo "========================================================================"
echo "ç»“æŸæ—¶é—´: $(date)"
echo "æ—¥å¿—æ–‡ä»¶: $LOG_FILE"
echo ""
echo "ğŸ“Š æŸ¥çœ‹ç»“æœï¼š"
echo "  - Quamba1 æ¨¡å‹: pretrained_models/quamba1/"
echo "  - Quamba2 æ¨¡å‹: pretrained_models/quamba2/"
echo "  - æ—¥å¿—ç›®å½•: logs/"
echo ""
echo "å®éªŒå®Œæˆç»Ÿè®¡ï¼š"
echo "  ğŸ”µ Quamba1 (Mamba1 W8A8, æ— é¢å¤–å‚æ•°): 8 ä¸ªå®éªŒ"
echo "  ğŸŸ¢ Quamba2 (Mamba2 W4/W8, æœ‰é¢å¤–å‚æ•°): 12 ä¸ªå®éªŒ"
echo "  æ€»è®¡: 20 ä¸ªå®éªŒ"
echo ""
echo "æ–‡ä»¶å¤¹ç»“æ„ï¼š"
echo "  pretrained_models/quamba1/default/    - Quamba1 é»˜è®¤é…ç½®"
echo "  pretrained_models/quamba1/pa-1/       - Quamba1 pa=1.0"
echo "  pretrained_models/quamba2/default/    - Quamba2 é»˜è®¤é…ç½®"
echo "  pretrained_models/quamba2/pa-1/       - Quamba2 pa=1.0"
echo "========================================================================"
