# Percentileå®éªŒæ—¥å¿—ç³»ç»Ÿä½¿ç”¨è¯´æ˜

## ğŸ“‹ åŠŸèƒ½æ¦‚è¿°

è¿™ä¸ªç³»ç»Ÿè‡ªåŠ¨è®°å½•æ¯æ¬¡é‡åŒ–å®éªŒä¸­percentileè£å‰ªå’Œreorderå¯¹æ¿€æ´»èŒƒå›´çš„å½±å“ï¼ŒåŒ…æ‹¬ï¼š

1. **å®éªŒå…ƒæ•°æ®**ï¼šæ—¶é—´ã€å‘½ä»¤è¡Œã€é…ç½®å‚æ•°
2. **Percentileè£å‰ªå‰å**ï¼šæ¯å±‚çš„min/max/rangeå˜åŒ–
3. **Reorderæ•ˆæœ**ï¼šé‡æ’åºå¯¹èŒƒå›´çš„æ”¹å–„
4. **æœ€ç»ˆç»“æœ**ï¼šaccuracyå’Œperplexity

æ‰€æœ‰å®éªŒè®°å½•åˆ°**åŒä¸€ä¸ªæ–‡ä»¶**ï¼š`logs/percentile_experiments.jsonl`

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. è¿è¡Œå®éªŒï¼ˆè‡ªåŠ¨è®°å½•ï¼‰

```bash
# æ­£å¸¸è¿è¡Œé‡åŒ–å®éªŒï¼Œæ—¥å¿—ä¼šè‡ªåŠ¨è®°å½•
python3 main.py pretrained_models/mambaOriginalHuggingfaceDownload/mamba-370m \
  --quantize \
  --w_bits 8 \
  --a_bits 8 \
  --batch_size 16 \
  --eval_zero_shot \
  --task_list lambada_openai \
  --pretrained_dir ./pretrained_models \
  --log_dir logs \
  --percentile_alpha 1.0
```

**å®éªŒç»“æŸåä¼šè‡ªåŠ¨ï¼š**
- âœ… è®°å½•é…ç½®å’Œå‘½ä»¤
- âœ… æ”¶é›†å‰3å±‚çš„æ¿€æ´»ç»Ÿè®¡
- âœ… è®°å½•æœ€ç»ˆaccuracy/perplexity
- âœ… è¿½åŠ åˆ° `logs/percentile_experiments.jsonl`
- âœ… æ‰“å°æ‘˜è¦åˆ°ç»ˆç«¯

---

### 2. æŸ¥çœ‹æ—¥å¿—

#### æŸ¥çœ‹æ‰€æœ‰å®éªŒ
```bash
python3 view_percentile_logs.py
```

#### æŸ¥çœ‹æœ€è¿‘5æ¬¡å®éªŒ
```bash
python3 view_percentile_logs.py --last 5
```

#### ç­›é€‰ç‰¹å®šæ¨¡å‹
```bash
python3 view_percentile_logs.py --filter "mamba-370m"
```

#### å¯¹æ¯”ä¸¤ä¸ªå®éªŒ
```bash
# å¯¹æ¯”ç¬¬0ä¸ªå’Œç¬¬1ä¸ªå®éªŒ
python3 view_percentile_logs.py --compare 0,1
```

---

## ğŸ“Š æ—¥å¿—æ ¼å¼è¯´æ˜

### JSONç»“æ„
```json
{
  "timestamp": "2025-11-05 15:30:00",
  "command": "python3 main.py ...",
  "config": {
    "model": "mamba-370m",
    "w_bits": 8,
    "a_bits": 8,
    "percentile_alpha": 1.0,
    "group_heads": false,
    "apply_gptq": true
  },
  "activation_stats": {
    "layer_0.x_proj:input": {
      "before_percentile": {
        "min": -127.35,
        "max": 156.82,
        "range": 284.17
      },
      "after_percentile": {
        "min": -127.35,
        "max": 156.82,
        "range": 284.17
      },
      "percentile_alpha": 1.0,
      "clipped_ratio": 0.0,
      "range_reduction": 0.0
    }
  },
  "reorder_summary": {
    "enabled": false,
    "avg_range_reduction": null,
    "total_layers": 0
  },
  "results": {
    "accuracy": 0.5205,
    "perplexity": 9.621
  }
}
```

---

## ğŸ” å…³é”®æŒ‡æ ‡è§£é‡Š

### 1. Percentileè£å‰ªæ•ˆæœ

**before_percentile**ï¼šçœŸå®æ¿€æ´»çš„min/max
```
min: -127.35
max: 156.82
range: 284.17  (max - min)
```

**after_percentile**ï¼špercentileè£å‰ªåçš„min/max
```
min: -120.50
max: 145.30
range: 265.80  (ç¼©å°äº†6.4%)
```

**range_reduction**ï¼šèŒƒå›´ç¼©å°æ¯”ä¾‹
```
range_reduction = (284.17 - 265.80) / 284.17 = 0.064 (6.4%)
```

---

### 2. Percentile_alphaå½±å“

| alphaå€¼ | å«ä¹‰ | è£å‰ªæ¯”ä¾‹ |
|---------|------|---------|
| 0.9995 | 99.95%ç™¾åˆ†ä½ | è£å‰ªtop 0.05% |
| 0.99999 | 99.999%ç™¾åˆ†ä½ | è£å‰ªtop 0.001% |
| 1.0 | 100%ç™¾åˆ†ä½ | ä¸è£å‰ª (0%) |

**clipped_ratio = 1.0 - percentile_alpha**

---

### 3. Reorderæ•ˆæœï¼ˆä»…Mamba2ï¼‰

**before_reorder**ï¼šèšç±»å‰çš„æ¿€æ´»èŒƒå›´
**after_reorder**ï¼šèšç±»åçš„æ¿€æ´»èŒƒå›´

```json
"reorder_summary": {
  "enabled": true,
  "avg_range_reduction": 15.6,  // å¹³å‡ç¼©å°15.6%
  "total_layers": 32
}
```

---

## ğŸ“ˆ å®é™…ä½¿ç”¨æ¡ˆä¾‹

### æ¡ˆä¾‹1ï¼šå¯¹æ¯”pa=1.0å’Œé»˜è®¤å€¼

```bash
# è¿è¡Œä¸¤æ¬¡å®éªŒ
python3 main.py mamba-370m --quantize --w_bits 8 --a_bits 8 ...
# (è‡ªåŠ¨è®°å½•ä¸ºå®éªŒ0)

python3 main.py mamba-370m --quantize --w_bits 8 --a_bits 8 --percentile_alpha 1.0 ...
# (è‡ªåŠ¨è®°å½•ä¸ºå®éªŒ1)

# å¯¹æ¯”ç»“æœ
python3 view_percentile_logs.py --compare 0,1
```

**è¾“å‡ºç¤ºä¾‹**ï¼š
```
ğŸ“Š å¯¹æ¯”: å®éªŒ#0 vs å®éªŒ#1
================================================================================

é…ç½®å¯¹æ¯”:
  é¡¹ç›®                      å®éªŒ#0                    å®éªŒ#1
  ---------------------------------------------------------------------------
  æ¨¡å‹                      mamba-370m               mamba-370m
  é‡åŒ–                      W8A8                     W8A8
  Percentile Alpha          default                  1.0 âš ï¸
  Group Heads               False                    False
  GPTQ                      True                     True

ğŸ¯ ç»“æœå¯¹æ¯”:
  Accuracy: 49.39% vs 52.05% (å·®å¼‚: +2.66%)
  Perplexity: 10.693 vs 9.621 (å·®å¼‚: -1.072)

ğŸ“Š æ¿€æ´»èŒƒå›´å¯¹æ¯”ï¼ˆç¬¬ä¸€å±‚ï¼‰:

  è£å‰ªå‰èŒƒå›´:
    å®éªŒ#0: 284.17
    å®éªŒ#1: 284.17

  è£å‰ªåèŒƒå›´:
    å®éªŒ#0: 265.80  (è£å‰ªäº†6.4%)
    å®éªŒ#1: 284.17  (ä¸è£å‰ª)
```

---

### æ¡ˆä¾‹2ï¼šåˆ†æReorderæ•ˆæœ

```bash
# Mamba2å®éªŒï¼ˆæœ‰reorderï¼‰
python3 main.py state-spaces/mamba2-2.7b \
  --quantize --group_heads \
  --w_bits 8 --a_bits 8 ...

# æŸ¥çœ‹æœ€è¿‘ä¸€æ¬¡å®éªŒ
python3 view_percentile_logs.py --last 1
```

**è¾“å‡ºç¤ºä¾‹**ï¼š
```
ğŸ“Š æ¿€æ´»ç»Ÿè®¡ (å‰3å±‚):

  layer_0.x_conv_out:input_reordered:
    è£å‰ªå‰: [-89.32, 102.45] èŒƒå›´=191.77
    è£å‰ªå: [-89.32, 102.45] èŒƒå›´=191.77
    è£å‰ªæ¯”ä¾‹: 0.0001%

ğŸ”„ Reorderæ•ˆæœ:
  å½±å“å±‚æ•°: 32
  å¹³å‡èŒƒå›´ç¼©å°: 15.6%
```

---

### æ¡ˆä¾‹3ï¼šç­›é€‰ç‰¹å®šæ¨¡å‹çš„æ‰€æœ‰å®éªŒ

```bash
# æŸ¥çœ‹æ‰€æœ‰mamba-370mçš„å®éªŒ
python3 view_percentile_logs.py --filter "mamba-370m"
```

---

## ğŸ¯ å¦‚ä½•å›ç­”ä½ çš„é—®é¢˜

### Q1: pa=1.0çš„min/maxæ˜¯å¤šå°‘ï¼Ÿ
**æŸ¥çœ‹æ—¥å¿—**ï¼š
```bash
python3 view_percentile_logs.py --filter "percentile_alpha: 1.0"
```

æŸ¥çœ‹ `activation_stats` ä¸­çš„ `after_percentile` å­—æ®µã€‚

---

### Q2: é»˜è®¤percentileçš„min/maxæ˜¯å¤šå°‘ï¼Ÿ
**æŸ¥çœ‹æ—¥å¿—**ï¼š
```bash
python3 view_percentile_logs.py --filter "percentile_alpha: null"
```

æˆ–è€…ç›´æ¥æ‰“å¼€ `logs/percentile_experiments.jsonl`ï¼Œæœç´¢æ²¡æœ‰è®¾ç½® `percentile_alpha` çš„å®éªŒã€‚

---

### Q3: Reorderæ”¹å–„äº†å¤šå°‘ï¼Ÿ
**æŸ¥çœ‹æ—¥å¿—**ï¼š
```bash
python3 view_percentile_logs.py --filter "group_heads: true"
```

æŸ¥çœ‹ `reorder_summary.avg_range_reduction` å­—æ®µã€‚

**æˆ–è€…å¯¹æ¯”**ï¼š
```bash
# æœ‰reorder vs æ— reorder
python3 view_percentile_logs.py --compare 0,1
```

---

### Q4: æœ€ç»ˆaccuracyæ˜¯å¤šå°‘ï¼Ÿ
**æŸ¥çœ‹æ—¥å¿—**ï¼š
```bash
python3 view_percentile_logs.py --last 5
```

æŸ¥çœ‹ `results.accuracy` å­—æ®µã€‚

---

## ğŸ”§ è‡ªå®šä¹‰å’Œæ‰©å±•

### åªè®°å½•ç‰¹å®šå±‚
ä¿®æ”¹ `quamba/modelutils_mamba.py` çš„æ”¶é›†ä»£ç ï¼š
```python
# åªè®°å½•å‰3å±‚
for i in range(min(3, len(layers))):
    ...

# æ”¹ä¸ºè®°å½•æ‰€æœ‰å±‚
for i in range(len(layers)):
    ...
```

### æ·»åŠ æ›´å¤šç»Ÿè®¡ä¿¡æ¯
ä¿®æ”¹ `quamba/observer.py` çš„ `get_stats()` æ–¹æ³•ï¼š
```python
def get_stats(self):
    return {
        "before_percentile": ...,
        "after_percentile": ...,
        # æ·»åŠ æ–°å­—æ®µ
        "median": torch.median(self.w_max).item(),
        "std": torch.std(self.w_max).item(),
    }
```

---

## ğŸ“‚ æ–‡ä»¶ä½ç½®

```
Quamba/
â”œâ”€â”€ logs/
â”‚   â””â”€â”€ percentile_experiments.jsonl  # ç»Ÿä¸€çš„å®éªŒæ—¥å¿—
â”œâ”€â”€ quamba/
â”‚   â”œâ”€â”€ percentile_logger.py           # Loggerå®ç°
â”‚   â”œâ”€â”€ observer.py                    # ä¿®æ”¹ï¼šæ”¶é›†ç»Ÿè®¡
â”‚   â””â”€â”€ modelutils_mamba.py            # ä¿®æ”¹ï¼šè°ƒç”¨logger
â”œâ”€â”€ main.py                            # ä¿®æ”¹ï¼šåˆå§‹åŒ–logger
â”œâ”€â”€ view_percentile_logs.py            # æŸ¥çœ‹å·¥å…·
â””â”€â”€ PERCENTILE_LOGGING.md              # æœ¬æ–‡æ¡£
```

---

## ğŸš¨ æ³¨æ„äº‹é¡¹

1. **æ—¥å¿—æ–‡ä»¶æ˜¯è¿½åŠ æ¨¡å¼**ï¼šæ¯æ¬¡å®éªŒéƒ½ä¼šè¿½åŠ åˆ° `percentile_experiments.jsonl`ï¼Œä¸ä¼šè¦†ç›–
2. **åªè®°å½•å‰3å±‚**ï¼šä¸ºäº†å‡å°‘æ—¥å¿—å¤§å°ï¼Œé»˜è®¤åªè®°å½•å‰3å±‚çš„æ¿€æ´»ç»Ÿè®¡
3. **JSONLæ ¼å¼**ï¼šæ¯è¡Œæ˜¯ä¸€ä¸ªç‹¬ç«‹çš„JSONå¯¹è±¡ï¼Œå¯ä»¥é€è¡Œè§£æ
4. **å¼‚å¸¸å¤„ç†**ï¼šå¦‚æœè®°å½•å¤±è´¥ï¼Œåªä¼šè­¦å‘Šï¼Œä¸ä¼šä¸­æ–­å®éªŒ

---

## ğŸ’¡ æœ€ä½³å®è·µ

### å®éªŒå‘½åè§„èŒƒ
åœ¨å‘½ä»¤ä¸­æ·»åŠ æè¿°æ€§å‚æ•°ï¼š
```bash
python3 main.py mamba-370m \
  --quantize \
  --percentile_alpha 1.0 \
  --output_subdir "exp_pa1.0_no_gptq" \
  ...
```

### å®šæœŸå¤‡ä»½æ—¥å¿—
```bash
cp logs/percentile_experiments.jsonl logs/percentile_experiments_backup_$(date +%Y%m%d).jsonl
```

### æ‰¹é‡å®éªŒ
```bash
# è¿è¡Œå¤šç»„å¯¹æ¯”å®éªŒ
for pa in 0.9995 0.99999 1.0; do
  python3 main.py mamba-370m \
    --quantize --percentile_alpha $pa ...
done

# ä¸€æ¬¡æ€§å¯¹æ¯”
python3 view_percentile_logs.py --last 3
```

---

## ğŸ“ ç¤ºä¾‹è¾“å‡º

### ç»ˆç«¯æ‘˜è¦è¾“å‡º
```
================================================================================
ğŸ“Š Percentileå®éªŒæ‘˜è¦
================================================================================

ğŸ”§ é…ç½®:
  æ¨¡å‹: mamba-370m
  é‡åŒ–: W8A8
  Percentile Alpha: 1.0
  Group Heads: False

ğŸ“ˆ æ¿€æ´»ç»Ÿè®¡ (å…±64å±‚):

  layer_0.x_proj:input:
    Percentileè£å‰ªå‰: [-127.35, 156.82] èŒƒå›´=284.17
    Percentileè£å‰ªå: [-127.35, 156.82] èŒƒå›´=284.17
    è¢«è£å‰ªæ¯”ä¾‹: 0.0000%

  å‰3å±‚å¹³å‡:
    è£å‰ªå‰å¹³å‡èŒƒå›´: 265.45
    è£å‰ªåå¹³å‡èŒƒå›´: 265.45
    èŒƒå›´ç¼©å°: 0.00%
    å¹³å‡è£å‰ªæ¯”ä¾‹: 0.0000%

ğŸ¯ æœ€ç»ˆç»“æœ:
  Accuracy: 52.05%
  Perplexity: 9.621

================================================================================

âœ… Percentileå®éªŒæ—¥å¿—å·²ä¿å­˜åˆ°: logs/percentile_experiments.jsonl
```

---

**ç¥å®éªŒé¡ºåˆ©ï¼ğŸ‰**
