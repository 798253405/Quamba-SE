# Mambaé‡åŒ–æ–¹æ³•æ–‡çŒ®ç»¼è¿°

**åˆ›å»ºæ—¶é—´**: 2025-11-05
**æ•°æ®æ¥æº**: arXivæœç´¢ + è®ºæ–‡åŸæ–‡
**ç›®çš„**: æ€»ç»“ç°æœ‰Mamba PTQæ–¹æ³•åŠQuambaçš„ä¼˜åŠ¿

---

## ğŸ“š ç°æœ‰Mambaé‡åŒ–æ–¹æ³•æ€»è§ˆ

### æ—¶é—´çº¿

```
2024-07  Mamba-PTQ        (é¦–æ¬¡æ¢ç´¢ï¼Œè¯†åˆ«outlieré—®é¢˜)
2024-10  Quamba          (é¦–ä¸ªå®Œæ•´Language Mamba PTQæ–¹æ¡ˆ)
2024-12  PTQ4VM          (Visual Mamba)
2025-01  QMamba          (Visual Mambaï¼Œ21%æå‡)
2025-01  MambaQuant      (KLT rotationæ–¹æ³•)
2025-03  Quamba2         (æ”¯æŒMamba2ï¼Œ3Ã—åŠ é€Ÿ)
```

---

## ğŸ”¬ å„æ–¹æ³•è¯¦ç»†åˆ†æ

### 1. Mamba-PTQ (Jul 2024)

**è®ºæ–‡**: *Mamba-PTQ: Outlier Channels in Recurrent Large Language Models* (arXiv 2407.12397)

**å…³é”®å‘ç°**:
- â­ **é¦–æ¬¡è¯†åˆ«**ï¼šMambaæ¨¡å‹å­˜åœ¨ä¸Transformerç±»ä¼¼çš„outlier channels
- é—®é¢˜ï¼šActivation outlierså¯¼è‡´é‡åŒ–å›°éš¾
- è´¡çŒ®ï¼šæä¾›baselineç»“æœï¼Œæå‡ºoutlier-awareé‡åŒ–çš„åˆæ­¥æ–¹æ¡ˆ

**å±€é™**:
- åªæ˜¯åˆæ­¥æ¢ç´¢ï¼ˆICML 2024 workshopï¼‰
- æ²¡æœ‰å®Œæ•´çš„è§£å†³æ–¹æ¡ˆ
- æ€§èƒ½æœªè¾¾åˆ°å®ç”¨æ°´å¹³

---

### 2. Quamba (Oct 2024)

**è®ºæ–‡**: *Quamba: A Post-Training Quantization Recipe for Selective State Space Models* (arXiv 2410.13229)

#### æ ¸å¿ƒè´¡çŒ®

**æŠ€æœ¯åˆ›æ–°**:
1. **Input Activationå¤„ç†**ï¼šPercentile clipping (99.999th)
   - æŠ‘åˆ¶selective SSMè¾“å…¥çš„æœ€å¤§å€¼
   - è·å¾—æ›´ç²¾ç»†çš„é‡åŒ–ç²¾åº¦

2. **Output Activationå¤„ç†**ï¼šHadamard transform
   - åœ¨outlier-freeç©ºé—´é‡åŒ–è¾“å‡º
   - Fused operationï¼Œæ— é¢å¤–å¼€é”€

**å¯¹æ¯”å®éªŒ** (Mamba 2.8B):

| æ–¹æ³• | Perplexity (â†“) | Zero-shot Acc (â†‘) | Latency (ms) |
|------|---------------|-----------------|-------------|
| FP16 Baseline | 9.45 | 63.1% | 103.56 |
| **SmoothQuant-SSM** | 13.59 | 57.3% | 56.53 |
| **QuaRot-SSM** | 9.89 | 62.4% | 67.76 |
| **Quamba** | **9.91** | **62.2%** | **60.17** |

**å…³é”®ä¼˜åŠ¿**:
- âœ… **ç²¾åº¦**: åªæœ‰0.9%å‡†ç¡®ç‡ä¸‹é™ï¼ˆvs FP16ï¼‰
- âœ… **é€Ÿåº¦**: 1.72Ã— speedup (vs FP16)
- âœ… **é²æ£’æ€§**: åœ¨Jamba-52Bæ··åˆæ¨¡å‹ä¸ŠæˆåŠŸï¼ˆnaiveæ–¹æ³•å¤±è´¥ï¼‰

**å±€é™**:
- âŒ åªæ”¯æŒMamba1
- âŒ åªæ”¯æŒW8A8
- âŒ æ²¡æœ‰é’ˆå¯¹Mamba2çš„ä¼˜åŒ–

---

### 3. MambaQuant (Jan 2025)

**è®ºæ–‡**: *MambaQuant: Quantizing the Mamba Family with Variance Aligned Rotation Methods* (arXiv 2501.13484)

#### æ ¸å¿ƒæŠ€æœ¯

**åˆ›æ–°æ–¹æ³•**:
1. **KLT-Enhanced Rotation**:
   - Karhunen-Loeve Transform
   - è‡ªé€‚åº”channelåˆ†å¸ƒçš„variance

2. **Smooth-Fused Rotation**:
   - å¹³è¡¡weightså’Œactivationsçš„channel variance
   - å°†é¢å¤–å‚æ•°èåˆåˆ°æ¨¡å‹æƒé‡

**è¯†åˆ«çš„é—®é¢˜**:
- Gate projectionsã€output projectionsã€matmulå­˜åœ¨æ˜¾è‘—outliers
- Parallel scanæœºåˆ¶æ”¾å¤§outliers
- äº§ç”Ÿä¸å‡åŒ€ã€é‡å°¾åˆ†å¸ƒ

**æ€§èƒ½**:
- âœ… W8A8ç²¾åº¦æŸå¤± < 1%ï¼ˆvision + languageï¼‰
- âœ… å¯¹æ¯”ï¼šQuaRotåœ¨Vim-Tä¸ŠæŸå¤±21%ï¼ŒMambaQuantå‡ ä¹æ— æŸ

**å®šä½**:
- "é¦–ä¸ªcomprehensive PTQ framework for Mamba family"

---

### 4. QMamba (Jan 2025)

**è®ºæ–‡**: *QMamba: Post-Training Quantization for Vision State Space Models* (arXiv 2501.13624)

#### é’ˆå¯¹Vision Mambaçš„æŒ‘æˆ˜

**è¯†åˆ«çš„é—®é¢˜**:
1. **ç¦»æ•£å‚æ•°åˆ†å¸ƒ**: Long-tailed skewness
2. **Hidden stateåŠ¨æ€æ€§**: é«˜åº¦åŠ¨æ€å˜åŒ–

**åˆ›æ–°æŠ€æœ¯**:
1. **Long-tailed Skewness Quantization (LtSQ)**:
   - å¤„ç†skewed distribution
   - å‡å°‘ç¦»æ•£å‚æ•°çš„é‡åŒ–è¯¯å·®

2. **Temporal Group Quantization (TGQ)**:
   - å¤„ç†hidden stateçš„åŠ¨æ€å˜åŒ–
   - æ—¶åºåˆ†ç»„é‡åŒ–

**æ€§èƒ½**:
- âœ… ImageNet 4-bit activation: **+21.0%** vs å…¶ä»–æ–¹æ³•
- âœ… å¤šç§æ¨¡å‹æ¶æ„å’Œå°ºå¯¸ä¸Šè¶…è¶Šç°æœ‰PTQæ–¹æ³•

**ç‰¹ç‚¹**:
- ä¸“é—¨é’ˆå¯¹Vision Mambaï¼ˆä¸æ˜¯Languageï¼‰
- é¦–ä¸ªVision SSM PTQæ¡†æ¶

---

### 5. PTQ4VM (Dec 2024)

**è®ºæ–‡**: *PTQ4VM: Post-Training Quantization for Visual Mamba* (arXiv 2412.20386)

#### é¦–ä¸ªVisual Mambaé‡åŒ–comprehensive study

**è¯†åˆ«çš„é—®é¢˜**:
1. Token-wise variance
2. Channel-wise outliers
3. Long tail of activations

**æŠ€æœ¯æ–¹æ³•**:
1. **Per-Token Static Quantization (PTS)**
2. **Joint Learning of Smoothing Scale and Step Size (JLSS)**

**å®šä½**:
- é¦–ä¸ªVisual Mambaé‡åŒ–çš„å…¨é¢ç ”ç©¶
- ä¸ºåç»­å·¥ä½œï¼ˆQMambaç­‰ï¼‰å¥ å®šåŸºç¡€

---

### 6. Quamba2 (Mar 2025)

**è®ºæ–‡**: *Quamba2: A Robust and Scalable Post-training Quantization Framework for Selective State Space Models* (arXiv 2503.22879)

#### Quambaçš„å‡çº§ç‰ˆ

**æ‰©å±•æ”¯æŒ**:
- âœ… Mamba1 + Mamba2
- âœ… W8A8, W4A8, W4A16

**æ ¸å¿ƒåˆ›æ–°**:
1. **Input Quantization**:
   - Sorting + Clustering
   - åˆ©ç”¨channel-order preservation
   - åˆ©ç”¨activation persistence

2. **Parameter Quantization**:
   - Per-state-groupé‡åŒ–ï¼ˆBå’ŒCå‚æ•°ï¼‰

3. **Compute-Invariantä¼˜åŒ–**:
   - Offlineé‡æ’æƒé‡
   - ä¿æŒSSM outputä¸€è‡´æ€§

**æ€§èƒ½æå‡** (Quamba2-8B):
- âœ… Prefilling: **1.3Ã— speedup**
- âœ… Generation: **3Ã— speedup**
- âœ… Memory: **4Ã— reduction**
- âœ… Accuracy: åªæŸå¤±1.6%

**å¯¹æ¯”**:
- è¶…è¶Š"two state-of-the-art SSM quantization methods"
- è®ºæ–‡æœªæ˜ç¡®æŒ‡å‡ºæ˜¯å“ªä¸¤ä¸ªï¼ˆå¯èƒ½æ˜¯MambaQuantå’ŒQMambaï¼‰

---

## ğŸ†š æ–¹æ³•å¯¹æ¯”æ€»ç»“

### Language Mamba PTQ

| æ–¹æ³• | æ—¶é—´ | æŠ€æœ¯è·¯çº¿ | ä¸»è¦ä¼˜åŠ¿ | å±€é™ |
|------|------|---------|---------|------|
| **Mamba-PTQ** | 2024-07 | Outlierè¯†åˆ« | é¦–æ¬¡æ¢ç´¢ | ä¸å®Œæ•´ |
| **Quamba** | 2024-10 | Percentile + Hadamard | ç²¾åº¦+é€Ÿåº¦å¹³è¡¡ | åªæ”¯æŒMamba1 W8A8 |
| **MambaQuant** | 2025-01 | KLT rotation | <1%ç²¾åº¦æŸå¤± | è®¡ç®—å¼€é”€ï¼Ÿ |
| **Quamba2** | 2025-03 | Clustering + Piecewise | **3Ã—åŠ é€Ÿï¼Œæ”¯æŒå¤šé…ç½®** | - |

### Vision Mamba PTQ

| æ–¹æ³• | æ—¶é—´ | æŠ€æœ¯è·¯çº¿ | ä¸»è¦ä¼˜åŠ¿ |
|------|------|---------|---------|
| **PTQ4VM** | 2024-12 | PTS + JLSS | é¦–ä¸ªcomprehensive study |
| **QMamba** | 2025-01 | LtSQ + TGQ | **+21%ç²¾åº¦ (4-bit)** |

---

## ğŸ¯ Quambaç³»åˆ—çš„ç‹¬ç‰¹ä¼˜åŠ¿

### 1. **é¦–ä¸ªLanguage Mambaå®Œæ•´æ–¹æ¡ˆ** (Quamba)

**vs Mamba-PTQ**:
- âœ… å®Œæ•´è§£å†³æ–¹æ¡ˆï¼ˆä¸åªæ˜¯è¯†åˆ«é—®é¢˜ï¼‰
- âœ… å®ç”¨æ€§èƒ½ï¼ˆ0.9%ç²¾åº¦æŸå¤±ï¼Œ1.72Ã—åŠ é€Ÿï¼‰
- âœ… åœ¨Jambaæ··åˆæ¨¡å‹ä¸ŠæˆåŠŸ

**vs SmoothQuant/QuaRotæ”¹ç¼–ç‰ˆ**:
- âœ… SSMç‰¹å®šè®¾è®¡ï¼ˆä¸æ˜¯Transformeræ”¹ç¼–ï¼‰
- âœ… æ›´å¥½çš„ç²¾åº¦-é€Ÿåº¦æƒè¡¡
- âœ… æ›´ä½çš„overheadï¼ˆfused operationsï¼‰

### 2. **ç³»ç»Ÿçº§ä¼˜åŒ–** (Quamba2)

**vs MambaQuant**:
- âœ… æ›´å¿«çš„æ¨ç†é€Ÿåº¦ï¼ˆ3Ã— generationï¼‰
- âœ… æ”¯æŒå¤šç§bit-widthé…ç½®ï¼ˆW4A8, W4A16ï¼‰
- âœ… åŒæ—¶æ”¯æŒMamba1å’ŒMamba2

**æŠ€æœ¯å·®å¼‚**:
```
MambaQuantè·¯çº¿ï¼š
  Rotation-based (KLT + Smooth-fused)
  â†’ ç²¾åº¦ä¼˜å…ˆï¼ˆ<1%æŸå¤±ï¼‰
  â†’ å¯èƒ½æœ‰rotationè®¡ç®—å¼€é”€

Quamba2è·¯çº¿ï¼š
  Clustering-based (Sorting + Piecewise scale)
  â†’ é€Ÿåº¦ä¼˜å…ˆï¼ˆ3Ã—åŠ é€Ÿï¼‰
  â†’ Offlineä¼˜åŒ–ï¼Œruntimeæ— é¢å¤–å¼€é”€
```

### 3. **éƒ¨ç½²å‹å¥½æ€§**

**Quambaç³»åˆ—çš„å·¥ç¨‹ä¼˜åŠ¿**:
- âœ… **Fused operations**: Hadamard transformèåˆï¼Œæ— é¢å¤–å¼€é”€
- âœ… **Static quantization**: å®Œå…¨é™æ€ï¼Œæ— runtimeè®¡ç®—
- âœ… **è¾¹ç¼˜è®¾å¤‡å®æµ‹**: Orin Nano 8Gå®æ—¶ç”Ÿæˆï¼ˆ13 tokens/secï¼‰
- âœ… **å¼€æºå®Œæ•´**: ä»£ç  + é¢„è®­ç»ƒæ¨¡å‹ + CUDA kernels

**å¯¹æ¯”**:
- MambaQuant: å­¦æœ¯æ–¹æ³•ï¼Œæœªè§éƒ¨ç½²æ•°æ®
- QMamba: é’ˆå¯¹Visionï¼Œä¸æ˜¯Language
- PTQ4VM: é’ˆå¯¹Visual Mamba

---

## ğŸ“Š æ€§èƒ½æ•°æ®æ±‡æ€»

### Language Mamba 2.8B (Zero-shot Average)

| æ–¹æ³• | Accuracy | vs FP16 | Latency (Orin Nano) | Speedup |
|------|----------|---------|-------------------|---------|
| FP16 Baseline | 63.1% | - | 103.56 ms | 1.0Ã— |
| SmoothQuant-SSM | 57.3% | -5.8% | 56.53 ms | 1.83Ã— |
| QuaRot-SSM | 62.4% | -0.7% | 67.76 ms | 1.53Ã— |
| **Quamba** | **62.2%** | **-0.9%** | **60.17 ms** | **1.72Ã—** |

### Mamba2-8B (Quamba2)

| é˜¶æ®µ | Speedup | Memory | Accuracy Drop |
|------|---------|--------|--------------|
| Prefilling | 1.3Ã— | 4Ã— reduction | 1.6% |
| Generation | **3Ã—** | 4Ã— reduction | 1.6% |

### Vision Mamba (QMamba, 4-bit activation)

| æ–¹æ³• | ImageNet Acc | Improvement |
|------|-------------|-------------|
| Existing PTQ | ~XX% | - |
| **QMamba** | **+21.0%** | æ˜¾è‘—æå‡ |

---

## ğŸ’¡ æŠ€æœ¯è·¯çº¿å¯¹æ¯”

### Rotation-basedæ–¹æ³•

**ä»£è¡¨**: MambaQuant, QuaRot-SSM

**åŸç†**:
```
æ¿€æ´»å€¼ â†’ Hadamard/KLT rotation â†’ é‡åŒ– â†’ årotation
```

**ä¼˜åŠ¿**:
- âœ… ç²¾åº¦é«˜ï¼ˆç†è®ºä¸Šoptimalï¼‰
- âœ… é€šç”¨æ€§å¼º

**åŠ£åŠ¿**:
- âš ï¸ Runtimeå¼€é”€ï¼ˆrotationè®¡ç®—ï¼‰
- âš ï¸ å†…å­˜å¼€é”€ï¼ˆrotationçŸ©é˜µï¼‰

### Smoothing-basedæ–¹æ³•

**ä»£è¡¨**: SmoothQuant-SSM

**åŸç†**:
```
æ¿€æ´»å€¼ â†’ Smoothing (scale equalization) â†’ é‡åŒ–
```

**ä¼˜åŠ¿**:
- âœ… ç®€å•
- âœ… å¼€é”€å°

**åŠ£åŠ¿**:
- âŒ ç²¾åº¦æŸå¤±å¤§ï¼ˆMambaä¸Š-5.8%ï¼‰
- âŒ ä¸é€‚åˆSSMç‰¹æ€§

### Clustering-basedæ–¹æ³•

**ä»£è¡¨**: Quamba2

**åŸç†**:
```
Offline: åˆ†ææ¿€æ´»å€¼åˆ†å¸ƒ â†’ Clustering â†’ ç”Ÿæˆpiecewise scales
Runtime: Lookup scale â†’ é‡åŒ–ï¼ˆæ— é¢å¤–è®¡ç®—ï¼‰
```

**ä¼˜åŠ¿**:
- âœ… **Runtimeæ— å¼€é”€**ï¼ˆscalesé¢„è®¡ç®—ï¼‰
- âœ… **é€Ÿåº¦å¿«**ï¼ˆ3Ã— generationï¼‰
- âœ… **ç»†ç²’åº¦æ§åˆ¶**ï¼ˆ128ä¸ªscalesï¼‰

**åŠ£åŠ¿**:
- âš ï¸ ä¾èµ–Calibrationè´¨é‡
- âš ï¸ é™æ€scaleï¼ˆåˆ†å¸ƒåç§»æ—¶æ€§èƒ½ä¸‹é™ï¼‰

### Percentile-basedæ–¹æ³•

**ä»£è¡¨**: Quamba (original)

**åŸç†**:
```
è¾“å…¥æ¿€æ´»å€¼ â†’ Percentile clipping (99.999th) â†’ é‡åŒ–
è¾“å‡ºæ¿€æ´»å€¼ â†’ Hadamard transform â†’ é‡åŒ–
```

**ä¼˜åŠ¿**:
- âœ… ç®€å•æœ‰æ•ˆ
- âœ… SSMç‰¹å®šä¼˜åŒ–

**åŠ£åŠ¿**:
- âš ï¸ Percentileé€‰æ‹©æ•æ„Ÿï¼ˆæ‚¨çš„å‘ç°ï¼šalpha=1.0 > 0.9995ï¼‰

---

## ğŸ” Quambaçš„æ ¸å¿ƒå·®å¼‚åŒ–

### 1. **æ¶æ„ç‰¹å®šè®¾è®¡**

**å…¶ä»–æ–¹æ³•çš„é—®é¢˜**:
- SmoothQuant/QuaRot: ä¸ºTransformerè®¾è®¡ï¼Œæ”¹ç¼–åˆ°Mambaæ•ˆæœä¸ä½³
- é€šç”¨PTQæ–¹æ³•: æœªè€ƒè™‘SSMçš„selective scanç‰¹æ€§

**Quambaçš„ä¼˜åŠ¿**:
> "Existing quantization techniques are poorly suited for SSMs due to unique architectural characteristics"

- âœ… é’ˆå¯¹selective scançš„æ•æ„Ÿæ€§è®¾è®¡
- âœ… å¤„ç†SSMç‰¹æœ‰çš„outlier patternï¼ˆä¸åŒäºAttentionï¼‰
- âœ… åˆ©ç”¨SSMçš„activation persistence

### 2. **å®ç”¨æ€§ä¼˜å…ˆ**

**å­¦æœ¯vså·¥ç¨‹**:

| ç»´åº¦ | MambaQuant | QMamba | Quambaç³»åˆ— |
|------|-----------|--------|-----------|
| **ç²¾åº¦** | æœ€ä¼˜ (<1%) | ä¼˜ (+21%) | è‰¯ (~1.6%) |
| **é€Ÿåº¦** | æœªçŸ¥ | æœªçŸ¥ | **æœ€ä¼˜ (3Ã—)** |
| **éƒ¨ç½²** | æœªçŸ¥ | æœªçŸ¥ | **å®Œæ•´ (Orinå®æµ‹)** |
| **å¼€æº** | æœªçŸ¥ | æœªçŸ¥ | **ä»£ç +æ¨¡å‹+CUDA** |

**Quambaçš„å®šä½**:
- ä¸è¿½æ±‚ç†è®ºæœ€ä¼˜ç²¾åº¦
- **è¿½æ±‚å·¥ç¨‹å¯ç”¨æ€§**ï¼šé€Ÿåº¦ + ç²¾åº¦ + éƒ¨ç½²çš„å¹³è¡¡

### 3. **æ··åˆæ¨¡å‹æ”¯æŒ**

**Quambaåœ¨Jamba-52Bä¸Šçš„æˆåŠŸ**:
- Jamba = Transformer + Mambaæ··åˆæ¶æ„
- Quambaé‡åŒ–Mambaéƒ¨åˆ† + LLM.int8é‡åŒ–Transformeréƒ¨åˆ†
- **æˆåŠŸ**: åªæœ‰1.1%ç²¾åº¦æŸå¤±
- **å¯¹æ¯”**: Naive quantizationå®Œå…¨å¤±è´¥

**æ„ä¹‰**:
- è¯æ˜Quambaä¸å…¶ä»–é‡åŒ–æ–¹æ³•å…¼å®¹
- ä¸ºæ··åˆæ¶æ„é‡åŒ–æä¾›æ–¹æ¡ˆ

---

## ğŸ“ˆ ç ”ç©¶è¶‹åŠ¿åˆ†æ

### æ—¶é—´çº¿æ€»ç»“

```
2024-07  Mamba-PTQ     â†’ è¯†åˆ«é—®é¢˜
         â†“
2024-10  Quamba       â†’ é¦–ä¸ªå®Œæ•´æ–¹æ¡ˆï¼ˆLanguageï¼‰
         â†“
2024-12  PTQ4VM       â†’ æ¢ç´¢Vision Mamba
         â†“
2025-01  QMamba       â†’ Visionä¼˜åŒ– (+21%)
         MambaQuant   â†’ Rotationæ–¹æ³•
         â†“
2025-03  Quamba2      â†’ é€Ÿåº¦ä¼˜åŒ– (3Ã—)
```

### ç ”ç©¶åˆ†åŒ–

**ä¸¤æ¡è·¯çº¿**:
1. **Language Mamba**: Quambaç³»åˆ— vs MambaQuant
2. **Vision Mamba**: QMamba vs PTQ4VM

**æœªæ¥æ–¹å‘**:
- æ›´ä½bit-widthï¼ˆW4A4, W2A8ï¼‰
- æ··åˆæ¶æ„é‡åŒ–ï¼ˆJamba-likeï¼‰
- QAT for Mamba
- ç¡¬ä»¶åŠ é€Ÿå™¨ï¼ˆFPGA, ASICï¼‰

---

## ğŸ¯ å¯¹æ‚¨ç ”ç©¶çš„å¯ç¤º

### Quambaçš„çœŸå®ä¼˜åŠ¿

**ä¸æ˜¯**:
- âŒ æœ€é«˜ç²¾åº¦ï¼ˆMambaQuantå¯èƒ½æ›´å¥½ï¼‰
- âŒ æ–°çš„é‡åŒ–ç†è®º
- âŒ é€šç”¨æ–¹æ¡ˆ

**è€Œæ˜¯**:
- âœ… **é¦–ä¸ªLanguage Mambaå®Œæ•´æ–¹æ¡ˆ**
- âœ… **å·¥ç¨‹å®ç”¨æ€§**ï¼šé€Ÿåº¦+ç²¾åº¦+éƒ¨ç½²çš„æœ€ä½³å¹³è¡¡
- âœ… **SSMç‰¹å®šä¼˜åŒ–**ï¼šä¸æ˜¯Transformeræ”¹ç¼–
- âœ… **è¾¹ç¼˜è®¾å¤‡å®æµ‹**ï¼šOrin Nanoå®æ—¶ç”Ÿæˆ
- âœ… **å¼€æºç”Ÿæ€**ï¼šä»£ç +æ¨¡å‹+CUDA kernels

### æ”¹è¿›æ–¹å‘

åŸºäºæ–‡çŒ®åˆ†æï¼Œæ‚¨å¯ä»¥è€ƒè™‘ï¼š

1. **å€Ÿé‰´MambaQuantçš„rotationæ–¹æ³•**:
   - KLT-enhanced rotationå¯èƒ½æå‡ç²¾åº¦
   - ä½†éœ€è¦è¯„ä¼°runtimeå¼€é”€

2. **ä¼˜åŒ–Percentileç­–ç•¥**:
   - æ‚¨çš„å‘ç°ï¼šalpha=1.0 > 0.9995
   - Quambaç”¨99.999thï¼Œå¯èƒ½è¿‡äºä¿å®ˆ
   - å»ºè®®ï¼šPer-layer adaptive percentile

3. **ç»“åˆQMambaçš„temporalæ–¹æ³•**:
   - TGQå¤„ç†hidden stateåŠ¨æ€æ€§
   - å¯èƒ½é€‚ç”¨äºlanguage Mambaçš„é•¿åºåˆ—

4. **æ‰©å±•æ··åˆæ¶æ„æ”¯æŒ**:
   - Quambaåœ¨Jambaä¸Šçš„æˆåŠŸ
   - å¯ä»¥æ¢ç´¢æ›´å¤šæ··åˆæ¶æ„

---

## ğŸ“š å‚è€ƒæ–‡çŒ®

### Quambaç³»åˆ—
1. Chiang et al., "Quamba: A Post-Training Quantization Recipe for Selective State Space Models", arXiv:2410.13229, Oct 2024
2. Chiang et al., "Quamba2: A Robust and Scalable Post-training Quantization Framework for Selective State Space Models", arXiv:2503.22879, Mar 2025

### å…¶ä»–Mamba PTQ
3. "Mamba-PTQ: Outlier Channels in Recurrent Large Language Models", arXiv:2407.12397, Jul 2024
4. "MambaQuant: Quantizing the Mamba Family with Variance Aligned Rotation Methods", arXiv:2501.13484, Jan 2025

### Vision Mamba PTQ
5. "QMamba: Post-Training Quantization for Vision State Space Models", arXiv:2501.13624, Jan 2025
6. "PTQ4VM: Post-Training Quantization for Visual Mamba", arXiv:2412.20386, Dec 2024

### é€šç”¨æ–¹æ³•ï¼ˆMambaæ”¹ç¼–ï¼‰
7. SmoothQuant: Xiao et al., "SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models", 2023
8. QuaRot: Ashkboos et al., "QuaRot: Outlier-Free 4-Bit Inference in Rotated LLMs", 2024

---

**åˆ›å»ºæ—¶é—´**: 2025-11-05
**æ•°æ®æ¥æº**: arXivæœç´¢ (2024-2025)
**æ€»ç»“**: Quambaçš„ä¼˜åŠ¿åœ¨äº**é¦–ä¸ªLanguage Mambaå®Œæ•´æ–¹æ¡ˆ + å·¥ç¨‹å®ç”¨æ€§**ï¼Œè€Œéç†è®ºç²¾åº¦æœ€ä¼˜ã€‚
