# Outlierå¤„ç†çš„ç›¸å…³å·¥ä½œç»¼è¿°

**åˆ›å»ºæ—¶é—´**: 2025-11-05
**ç›®çš„**: æ•´ç†LLMé‡åŒ–ä¸­outlierå¤„ç†çš„å·²æœ‰æ–¹æ³•ï¼Œä¸ºQuambaçš„æ”¹è¿›æä¾›å‚è€ƒ

---

## ğŸ“š å·²æœ‰çš„Outlierå¤„ç†æ–¹æ³•

### 1. LLM.int8() - å¼€åˆ›æ€§å·¥ä½œ

**è®ºæ–‡**: Dettmers et al., "LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale", NeurIPS 2022

**æ ¸å¿ƒæ€è·¯**: Mixed-precision decompositionï¼ˆæ··åˆç²¾åº¦åˆ†è§£ï¼‰

```
çŸ©é˜µåˆ†è§£ï¼š
C = AB = (A_normal + A_outlier)(B_normal + B_outlier)
      â‰ˆ A_normal Â· B_normal (INT8) + A_outlier Â· B_outlier (FP16)
```

**å…³é”®æŠ€æœ¯**:
- **Outlierè¯†åˆ«**: ä½¿ç”¨é˜ˆå€¼Î±=6.0æ¥è¯†åˆ«outlierç»´åº¦
  - å¦‚æœæŸä¸ªfeatureç»´åº¦çš„å€¼ > 6.0ï¼Œæ ‡è®°ä¸ºoutlier
  - Outlierç»´åº¦çº¦å 0.1%
- **åˆ†ç¦»è®¡ç®—**:
  - 99.9%çš„normalå€¼ç”¨INT8é‡åŒ–
  - 0.1%çš„outlierç”¨FP16ä¿ç•™
- **Vector-wise quantization**: æŒ‰å‘é‡è€Œéæ•´ä¸ªtensoré‡åŒ–

**æ•ˆæœ**:
- åœ¨LLaMA-7Bä¸Šå‡ ä¹æ— ç²¾åº¦æŸå¤±
- å†…å­˜å ç”¨é™ä½50%

**å±€é™**:
- éœ€è¦æ··åˆç²¾åº¦è®¡ç®—ï¼ˆINT8 + FP16ï¼‰
- é¢å¤–çš„outlieræ£€æµ‹å’Œåˆ†ç¦»å¼€é”€
- éœ€è¦ç¡¬ä»¶æ”¯æŒFP16å’ŒINT8çš„æ··åˆè¿ç®—

**å‚è€ƒ**:
- è®ºæ–‡: https://arxiv.org/abs/2208.07339
- åšå®¢: https://huggingface.co/blog/hf-bitsandbytes-integration

---

### 2. SqueezeLLM - ç¨€ç–åˆ†è§£

**è®ºæ–‡**: Kim et al., "SqueezeLLM: Dense-and-Sparse Quantization", ICML 2024

**æ ¸å¿ƒæ€è·¯**: Dense-and-Sparse decompositionï¼ˆå¯†é›†-ç¨€ç–åˆ†è§£ï¼‰

```
æƒé‡åˆ†è§£ï¼š
W = W_dense (low-bit) + W_sparse (full precision)
```

**å…³é”®æŠ€æœ¯**:
- **Sensitivity-based selection**:
  - åŸºäºHessiançŸ©é˜µè¯†åˆ«æ•æ„Ÿçš„æƒé‡
  - ä¸åªçœ‹magnitudeï¼Œè¿˜çœ‹å¯¹lossçš„å½±å“
- **æä½ç¨€ç–åº¦**: ä»…æå–0.45%çš„æƒé‡ä½œä¸ºç¨€ç–æˆåˆ†ï¼ˆæ¯”LLM.int8()æ›´å°‘ï¼‰
- **é«˜æ•ˆç¨€ç–å­˜å‚¨**:
  - ä½¿ç”¨CSR/CSCæ ¼å¼å­˜å‚¨ç¨€ç–çŸ©é˜µ
  - ä¼˜åŒ–çš„ç¨€ç–çŸ©é˜µä¹˜æ³•kernel
- **éå‡åŒ€é‡åŒ–**: å¯¹denseéƒ¨åˆ†ä½¿ç”¨non-uniform quantization

**æ•ˆæœ**:
- åœ¨3-bité‡åŒ–ä¸‹æ¥è¿‘FP16ç²¾åº¦
- æ¯”LLM.int8()çš„ç¨€ç–åº¦æ›´ä½ï¼ˆ0.45% vs 0.1%ï¼‰

**å±€é™**:
- Sensitivityè®¡ç®—éœ€è¦Hessianï¼ˆæˆæœ¬é«˜ï¼‰
- ç¨€ç–çŸ©é˜µè¿ç®—çš„ç¡¬ä»¶æ”¯æŒæœ‰é™
- éœ€è¦ä¸“é—¨çš„ç¨€ç–kernel

**å‚è€ƒ**:
- è®ºæ–‡: https://arxiv.org/abs/2306.07629
- ä»£ç : https://github.com/SqueezeAILab/SqueezeLLM

---

### 3. AWQ - Activation-aware Weight Quantization

**è®ºæ–‡**: Lin et al., "AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration", MLSys 2024

**æ ¸å¿ƒæ€è·¯**: ä¿æŠ¤é‡è¦çš„æƒé‡é€šé“

```
è¯†åˆ«é‡è¦é€šé“ï¼š
importance = ||activation_channel||â‚‚
ä¿ç•™top 0.1%ä¸ºFP16
```

**å…³é”®æŠ€æœ¯**:
- **Activation-aware**: æ ¹æ®activationçš„magnitudeè¯†åˆ«é‡è¦æƒé‡
- **Per-channel scaling**: å¯¹ä¸åŒchannelä½¿ç”¨ä¸åŒscale
- **0.1% FP16ä¿ç•™**: æœ€é‡è¦çš„0.1%æƒé‡ä¿ç•™FP16

**æ•ˆæœ**:
- W4A16åœ¨LLaMAä¸Šå‡ ä¹æ— æŸ
- æ¯”GPTQæ›´å¿«ï¼ˆæ— éœ€Hessianï¼‰

**å±€é™**:
- ä»ç„¶æ˜¯æ··åˆç²¾åº¦
- Activationéœ€è¦FP16ï¼ˆä¸æ˜¯W4A4ï¼‰

**å‚è€ƒ**:
- è®ºæ–‡: https://arxiv.org/abs/2306.00978
- åšå®¢: https://towardsdatascience.com/

---

### 4. ATOM - åŠ¨æ€Outlieré€‰æ‹©

**è®ºæ–‡**: Zhao et al., "ATOM: Low-bit Quantization for Efficient and Accurate LLM Serving", MLSys 2024

**æ ¸å¿ƒæ€è·¯**: åŠ¨æ€é‡æ’åºé€‰æ‹©outliers

```
åŠ¨æ€æµç¨‹ï¼š
1. å¯¹activationçŸ©é˜µé‡æ’åº
2. æŒ‘é€‰top-Kä¸ªoutliers
3. Normalå€¼ç”¨group quantizationï¼ˆä½bitï¼‰
4. Outliersç”¨é«˜bitç²¾åº¦
```

**å…³é”®æŠ€æœ¯**:
- **åŠ¨æ€é‡æ’åº**: Runtimeæ—¶æ ¹æ®activationåŠ¨æ€é€‰æ‹©outlier
- **Group quantization**: å¯¹normalå€¼åˆ†ç»„é‡åŒ–
- **æ··åˆbitå®½**: Normalå€¼4-bitï¼ŒOutlier 8-bit

**æ•ˆæœ**:
- W4A4åœ¨å¤§éƒ¨åˆ†ä»»åŠ¡ä¸Š<1%ç²¾åº¦æŸå¤±
- æ¯”LLM.int8()æ›´æ¿€è¿›çš„é‡åŒ–

**å±€é™**:
- Runtimeéœ€è¦åŠ¨æ€é‡æ’åºï¼ˆå¼€é”€ï¼‰
- ä»ç„¶æ˜¯æ··åˆç²¾åº¦

**å‚è€ƒ**:
- è®ºæ–‡: https://proceedings.mlsys.org/paper_files/paper/2024/hash/

---

### 5. OWQ - ç»“æ„åŒ–Outlieré‡åŒ–

**è®ºæ–‡**: Lee et al., "OWQ: Lessons learned from activation outliers for weight quantization in large language models", AAAI 2024

**æ ¸å¿ƒæ€è·¯**: ç»“æ„åŒ–çš„æ··åˆç²¾åº¦

```
æƒé‡åˆ†å—ï¼š
W = [Wâ‚, Wâ‚‚, ..., Wâ‚™]
å¯¹outlier-sensitiveçš„å—ä½¿ç”¨é«˜ç²¾åº¦
```

**å…³é”®æŠ€æœ¯**:
- **ç»“æ„åŒ–åˆ†å—**: æŒ‰ç…§ç»“æ„ï¼ˆå¦‚attention headsï¼‰åˆ†å—
- **å—çº§æ··åˆç²¾åº¦**: æ•´ä¸ªå—ç»Ÿä¸€ç²¾åº¦ï¼ˆè€Œéé›¶æ•£çš„å…ƒç´ ï¼‰
- **ç¡¬ä»¶å‹å¥½**: é¿å…ç»†ç²’åº¦çš„æ··åˆç²¾åº¦

**æ•ˆæœ**:
- åœ¨W3ä¸Šæ¥è¿‘FP16
- æ¯”éç»“æ„åŒ–æ–¹æ³•æ›´ç¡¬ä»¶å‹å¥½

**å‚è€ƒ**:
- ä»£ç : https://github.com/xvyaward/owq

---

### 6. MixLLM - Salience-basedæ··åˆç²¾åº¦

**è®ºæ–‡**: MixLLM, 2024

**æ ¸å¿ƒæ€è·¯**: åŸºäºsalienceçš„float16 + low-bitæ··åˆ

```
è§‚å¯Ÿï¼š
é«˜salienceå…ƒç´ å€¾å‘äºæ²¿output channelsåˆ†å¸ƒ
â†’ å¯ä»¥per-channelå¤„ç†
```

**å…³é”®æŠ€æœ¯**:
- **Salienceè®¡ç®—**: åŸºäºgradientæˆ–activation magnitude
- **Float16ä¿ç•™**: é«˜salienceå…ƒç´ ç”¨float16
- **Channel-wise pattern**: åˆ©ç”¨outlierçš„ç»“æ„åŒ–ç‰¹æ€§

**æ•ˆæœ**:
- æ›´å¥½çš„ç¡¬ä»¶locality
- å‡å°‘ç»†ç²’åº¦çš„æ··åˆç²¾åº¦

---

## ğŸ” æ–¹æ³•å¯¹æ¯”

| æ–¹æ³• | Outlierè¡¨ç¤º | ç¨€ç–åº¦ | é’ˆå¯¹å¯¹è±¡ | ç¡¬ä»¶å‹å¥½æ€§ | å‘è¡¨å¹´ä»½ |
|------|-----------|--------|---------|-----------|---------|
| **LLM.int8()** | FP16 | 0.1% | Activation | âš ï¸ éœ€æ··åˆç²¾åº¦ | 2022 |
| **SqueezeLLM** | Full precision | 0.05-0.45% | Weight | âš ï¸ éœ€ç¨€ç–è¿ç®— | 2023 |
| **AWQ** | FP16 | 0.1% | Weight | âš ï¸ éœ€æ··åˆç²¾åº¦ | 2023 |
| **ATOM** | 8-bit | åŠ¨æ€ | Activation | âš ï¸ Runtimeå¼€é”€ | 2024 |
| **OWQ** | é«˜bit | ç»“æ„åŒ– | Weight | âœ… ç»“æ„åŒ–å‹å¥½ | 2024 |
| **MixLLM** | Float16 | Channel-wise | Activation | âš ï¸ éœ€æ··åˆç²¾åº¦ | 2024 |

---

## ğŸ¯ Quambaä¸ç°æœ‰å·¥ä½œçš„åŒºåˆ«

### 1. æ¶æ„å·®å¼‚

| ç‰¹æ€§ | ç°æœ‰å·¥ä½œï¼ˆTransformerï¼‰ | Quambaï¼ˆMambaï¼‰ |
|------|----------------------|----------------|
| **Outlieræ¥æº** | Attentionçš„softmaxè¾“å‡º | SSMçŠ¶æ€çš„ç´¯ç§¯ |
| **åˆ†å¸ƒç‰¹æ€§** | ç¨€ç–ï¼ˆ0.1%ï¼‰ | å¯èƒ½æ›´å¯†é›† |
| **æ—¶åºä¾èµ–** | å•tokenå†… | è·¨tokenç´¯ç§¯ |

### 2. è¡¨è¾¾æ–¹å¼å·®å¼‚

#### ç°æœ‰å·¥ä½œï¼šFP16 + INT8æ··åˆ

```cuda
// LLM.int8()é£æ ¼
if (is_outlier[idx]) {
    result = fp16_compute(a_fp16, b_fp16);  // FP16è·¯å¾„
} else {
    result = int8_compute(a_int8, b_int8);  // INT8è·¯å¾„
}
// âŒ éœ€è¦åˆ†æ”¯ï¼Œéœ€è¦ä¸¤ç§è®¡ç®—è·¯å¾„
```

#### Quambaå½“å‰ï¼šå•ä¸€INT8

```cuda
// Quambaå½“å‰
int8_t q = clamp(round(x / scale), -128, 127);
result = int8_compute(q);  // ç»Ÿä¸€INT8
// âœ… æ— åˆ†æ”¯ï¼Œå•ä¸€è®¡ç®—è·¯å¾„
// âŒ Outlierè¢«clampï¼Œä¿¡æ¯ä¸¢å¤±
```

#### å¯èƒ½çš„Quambaæ”¹è¿›æ–¹å‘ï¼Ÿ

**æ–¹å‘Aï¼šçº¯INT8ï¼Œæ™ºèƒ½scale**
```python
# ä¿æŒå•ä¸€INT8ï¼Œä½†ç”¨æ›´å¥½çš„scale
scale = choose_robust_scale(activations)  # å¦‚alpha=1.0
# âœ… ç¡¬ä»¶å‹å¥½
# âš ï¸ Outlierä»ä¼šclampï¼ˆä½†å®éªŒæ˜¾ç¤ºå¯æ¥å—ï¼‰
```

**æ–¹å‘Bï¼šç»“æ„åŒ–æ··åˆç²¾åº¦**
```python
# å€Ÿé‰´OWQï¼ŒæŒ‰groupæ··åˆç²¾åº¦
for group in groups:
    if group_has_outliers(group):
        group.precision = 16  # FP16
    else:
        group.precision = 8   # INT8
# âš ï¸ éœ€è¦ç¡¬ä»¶æ”¯æŒ
# âœ… æ¯”ç»†ç²’åº¦æ··åˆæ›´å‹å¥½
```

**æ–¹å‘Cï¼šåŒINTè¡¨è¾¾ï¼ˆåŸåˆ›ï¼Ÿï¼‰**
```python
# ç”¨ä¸¤ä¸ªINT8è¡¨è¾¾ä¸€ä¸ªå€¼
q_coarse = round(x / scale_coarse)  # ç²—ç²’åº¦
q_fine = round((x - q_coarse * scale_coarse) / scale_fine)  # æ®‹å·®

# é‡å»º
x_approx = q_coarse * scale_coarse + q_fine * scale_fine

# âœ… çº¯æ•´æ•°
# âš ï¸ éœ€è¦éªŒè¯ç¡¬ä»¶å¼€é”€
```

---

## ğŸ“Š æ ¸å¿ƒTrade-offåˆ†æ

### ç²¾åº¦ vs ç¡¬ä»¶æ•ˆç‡

```
FP16+INT8æ··åˆç²¾åº¦ï¼š
  âœ… ç²¾åº¦æœ€é«˜ï¼ˆoutlieræ— æŸï¼‰
  âŒ éœ€è¦æ··åˆç²¾åº¦ç¡¬ä»¶
  âŒ åˆ†æ”¯åˆ¤æ–­å½±å“æµæ°´çº¿
  âŒ å†…å­˜è®¿é—®ä¸è¿ç»­

å•ä¸€INT8+æ™ºèƒ½scaleï¼š
  âš ï¸ ç²¾åº¦ä¸­ç­‰ï¼ˆoutlieræœ‰æŸï¼‰
  âœ… ç¡¬ä»¶å‹å¥½ï¼ˆTensor Coreï¼‰
  âœ… æ— åˆ†æ”¯
  âœ… å†…å­˜è¿ç»­
  âœ… å®éªŒæ˜¾ç¤ºï¼šæŸå¤±å¯æ¥å—ï¼ˆä½ çš„alpha=1.0å®éªŒï¼‰

ç»“æ„åŒ–æ··åˆç²¾åº¦ï¼š
  âœ… ç²¾åº¦é«˜
  âš ï¸ ç¡¬ä»¶ä¸­ç­‰å‹å¥½
  âœ… åˆ†æ”¯è¾ƒå°‘
  âš ï¸ éœ€è¦ä¸“é—¨æ”¯æŒ
```

---

## ğŸ’¡ å…³é”®Insightï¼šMSEä¸è¾“å…¥çš„å…³ç³»

### ä½ çš„è§‚å¯Ÿï¼ˆé‡è¦ï¼ï¼‰

> "æˆ‘ä¸æ˜¯è§‰å¾—MSEé”™çš„ï¼Œè€Œæ˜¯MSEå’Œè¾“å…¥æ˜¯ä¸æ˜¯ç›¸å…³ï¼Œæ¢ä¸ªåŸºå‡†å°±ä¼šå˜åŒ–"

**é—®é¢˜åˆ†æ**ï¼š

```python
# Calibrationé˜¶æ®µï¼ˆPileæ•°æ®ï¼‰
activations_pile = [...]
scale_optimal_pile = argmin_scale MSE(activations_pile, scale)
# â†’ å¾—åˆ°scale_A

# Testé˜¶æ®µï¼ˆLambadaæ•°æ®ï¼‰
activations_lambada = [...]  # åˆ†å¸ƒä¸åŒï¼
MSE_lambada = compute_MSE(activations_lambada, scale_A)
# â†’ MSEå¯èƒ½ä¸æ˜¯æœ€ä¼˜

# é—®é¢˜ï¼šåœ¨Pileä¸Šæœ€ä¼˜çš„scaleï¼Œåœ¨Lambadaä¸Šå¯èƒ½æ¬¡ä¼˜
```

**è¿™è§£é‡Šäº†ä¸ºä»€ä¹ˆalpha=1.0æ›´å¥½**ï¼š

```python
# alpha=0.9995ï¼ˆpercentileï¼‰
scale = percentile(pile_data, 0.9995) / 127
# â†’ é’ˆå¯¹Pileä¼˜åŒ–ï¼Œå¯èƒ½è¿‡æ‹Ÿåˆ

# alpha=1.0ï¼ˆmaxï¼‰
scale = max(pile_data) / 127
# â†’ æ›´ä¿å®ˆï¼Œæ›´é²æ£’ï¼Œé€‚ç”¨äºæ›´å¤šåˆ†å¸ƒ
```

### å·²æœ‰å·¥ä½œçš„ç±»ä¼¼å‘ç°

#### LLM.int8()çš„å‘ç°

```
è§‚å¯Ÿï¼šOutlierç»´åº¦åœ¨ä¸åŒè¾“å…¥ä¸Šä¿æŒä¸€è‡´
â†’ å¯ä»¥é¢„å…ˆè¯†åˆ«
â†’ å›ºå®šè¿™äº›ç»´åº¦ç”¨FP16
```

#### AWQçš„å‘ç°

```
è§‚å¯Ÿï¼šé‡è¦çš„æƒé‡channelä¸activationç›¸å…³
â†’ éœ€è¦åœ¨calibrationæ—¶è§‚å¯Ÿactivation
â†’ é€‰æ‹©å¯¹å¤šæ•°è¾“å…¥éƒ½é‡è¦çš„channel
```

#### SqueezeLLMçš„å‘ç°

```
è§‚å¯Ÿï¼šSensitivityåŸºäºHessianï¼Œä¾èµ–æ•°æ®åˆ†å¸ƒ
â†’ éœ€è¦ä»£è¡¨æ€§çš„calibrationæ•°æ®
â†’ ä½¿ç”¨æ··åˆå¤šç§æ•°æ®é›†
```

### Quambaçš„å¯ç¤º

**ç­–ç•¥1ï¼šé²æ£’scaleé€‰æ‹©**
```python
# ä¸è¿½æ±‚å•ä¸€æ•°æ®é›†çš„æœ€ä¼˜MSE
# è€Œæ˜¯è¿½æ±‚è·¨æ•°æ®é›†çš„é²æ£’æ€§

def robust_scale(activations):
    # æµ‹è¯•ä¸åŒscaleåœ¨å¤šä¸ªåˆ†å¸ƒä¸Šçš„è¡¨ç°
    scales = [
        max(activations) / 127,           # æœ€é²æ£’
        quantile(activations, 0.9999) / 127,
        quantile(activations, 0.9995) / 127,
    ]

    # åœ¨å¤šä¸ªåˆ†å¸ƒä¸Šè¯„ä¼°
    distributions = [pile_data, wikitext_data, lambada_data]

    best_scale = None
    best_avg_mse = float('inf')

    for scale in scales:
        avg_mse = 0
        for dist in distributions:
            mse = compute_mse(dist, scale)
            avg_mse += mse
        avg_mse /= len(distributions)

        if avg_mse < best_avg_mse:
            best_avg_mse = avg_mse
            best_scale = scale

    return best_scale
```

**ç­–ç•¥2ï¼šå¤šæ•°æ®é›†Calibration**
```python
# ä¸åªåœ¨Pileä¸Šcalibration
calibration_data = mix_datasets([
    load_dataset("pile", samples=256),
    load_dataset("wikitext", samples=128),
    load_dataset("lambada", samples=128),
])

scale = compute_scale(calibration_data)
# â†’ åœ¨å¤šç§åˆ†å¸ƒä¸Šéƒ½reasonable
```

---

## ğŸ“ è®ºæ–‡å†™ä½œå»ºè®®

### Related Workç« èŠ‚ç»“æ„

```markdown
## 2. Related Work

### 2.1 Outlier-aware Quantization for Transformers

ç°æœ‰å·¥ä½œä¸»è¦é’ˆå¯¹Transformeræ¶æ„ï¼Œé‡‡ç”¨æ··åˆç²¾åº¦æ–¹æ³•ï¼š

**Mixed-precision approaches**. LLM.int8() [Dettmers+22] é¦–æ¬¡ç³»ç»Ÿæ€§ç ”ç©¶
äº†LLMä¸­çš„outlierç°è±¡ï¼Œæå‡ºå°†0.1%çš„outlierç»´åº¦ä¿ç•™ä¸ºFP16ï¼Œå…¶ä½™é‡åŒ–
ä¸ºINT8ã€‚SqueezeLLM [Kim+23] è¿›ä¸€æ­¥é™ä½ç¨€ç–åº¦è‡³0.45%ï¼Œä½¿ç”¨Dense-and-
Sparseåˆ†è§£å’ŒåŸºäºsensitivityçš„éå‡åŒ€é‡åŒ–ã€‚AWQ [Lin+24] åŸºäºactivation
çš„magnitudeè¯†åˆ«é‡è¦æƒé‡ï¼Œä¿ç•™0.1%ä¸ºFP16ã€‚

**Dynamic outlier handling**. ATOM [Zhao+24] æå‡ºåŠ¨æ€é‡æ’åºactivation
çŸ©é˜µæ¥é€‰æ‹©outliersï¼Œå¯¹normalå€¼ä½¿ç”¨ä½bit group quantizationï¼Œå¯¹outliers
ä½¿ç”¨é«˜bitç²¾åº¦ã€‚

**Structured approaches**. OWQ [Lee+24] é‡‡ç”¨ç»“æ„åŒ–çš„æ··åˆç²¾åº¦ï¼Œé¿å…ç»†
ç²’åº¦çš„å…ƒç´ çº§æ··åˆï¼Œæ›´åŠ ç¡¬ä»¶å‹å¥½ã€‚

### 2.2 ä¸æœ¬å·¥ä½œçš„åŒºåˆ«

ä¸ç°æœ‰å·¥ä½œç›¸æ¯”ï¼Œæœ¬æ–‡æœ‰ä¸‰ä¸ªä¸»è¦åŒºåˆ«ï¼š

**æ¶æ„ç‰¹å¼‚æ€§**. ç°æœ‰å·¥ä½œé’ˆå¯¹Transformerçš„attentionæœºåˆ¶ï¼Œè€ŒMambaçš„
SSMçŠ¶æ€å…·æœ‰ä¸åŒçš„outlierç‰¹æ€§ï¼ˆæ—¶åºç´¯ç§¯ vs å•tokenç¨€ç–ï¼‰ã€‚

**è¡¨è¾¾æ–¹å¼**. ç°æœ‰å·¥ä½œé‡‡ç”¨FP16+INT8æ··åˆç²¾åº¦ï¼Œéœ€è¦ä¸“é—¨çš„ç¡¬ä»¶æ”¯æŒå’Œ
åˆ†æ”¯åˆ¤æ–­ã€‚æœ¬æ–‡æ¢ç´¢çº¯INT8æ–¹æ¡ˆï¼Œé€šè¿‡æ™ºèƒ½scaleé€‰æ‹©åœ¨å•ä¸€ç²¾åº¦ä¸‹å¤„ç†
outliersï¼Œæ›´åŠ ç¡¬ä»¶å‹å¥½ã€‚

**è·¨åˆ†å¸ƒé²æ£’æ€§**. æˆ‘ä»¬å‘ç°åœ¨calibrationæ•°æ®ä¸Šæœ€ä¼˜çš„scaleå¯èƒ½åœ¨test
æ•°æ®ä¸Šæ¬¡ä¼˜ã€‚å®éªŒè¡¨æ˜æ›´ä¿å®ˆçš„scaleé€‰æ‹©ï¼ˆalpha=1.0 vs 0.9995ï¼‰è™½ç„¶
åœ¨calibrationæ•°æ®ä¸ŠMSEç•¥é«˜ï¼Œä½†åœ¨diverse benchmarksä¸Šaccuracyæ›´å¥½
ï¼ˆ53.74% vs 53.2%ï¼‰ï¼Œè¯´æ˜é²æ£’æ€§æ¯”å•ä¸€æ•°æ®é›†çš„MSEä¼˜åŒ–æ›´é‡è¦ã€‚
```

### Contributionséƒ¨åˆ†

```markdown
## 1.2 Contributions

- **é¦–æ¬¡ç³»ç»Ÿç ”ç©¶SSMæ¶æ„çš„outlierç‰¹æ€§**ï¼Œå‘ç°å…¶ä¸Transformerçš„å·®å¼‚

- **æå‡ºçº¯INT8çš„outlier-awareé‡åŒ–æ–¹æ³•**ï¼Œæ— éœ€æ··åˆç²¾åº¦ç¡¬ä»¶ï¼Œä¿æŒ
  Tensor Coreå…¼å®¹æ€§

- **å‘ç°è·¨æ•°æ®é›†é²æ£’æ€§çš„é‡è¦æ€§**ï¼Œå®éªŒè¡¨æ˜ä¿å®ˆçš„scaleé€‰æ‹©ï¼ˆalpha=1.0ï¼‰
  åœ¨å¤šä¸ªbenchmarksä¸Šä¼˜äºé’ˆå¯¹calibrationæ•°æ®ä¼˜åŒ–çš„scaleï¼ˆalpha=0.9995ï¼‰
```

---

## ğŸ”¬ æœªæ¥ç ”ç©¶æ–¹å‘

### 1. SSM-specific Outlieråˆ†æ

```python
ç ”ç©¶é—®é¢˜ï¼š
- Mambaçš„outlierä¸Transformeræœ‰ä½•ä¸åŒï¼Ÿ
- SSMçŠ¶æ€çš„ç´¯ç§¯æ•ˆåº”å¦‚ä½•å½±å“outlieråˆ†å¸ƒï¼Ÿ
- ä¸åŒå±‚çš„outlierç‰¹æ€§æ˜¯å¦ä¸åŒï¼Ÿ
```

### 2. çº¯INT8çš„æé™

```python
ç ”ç©¶é—®é¢˜ï¼š
- çº¯INT8èƒ½è¾¾åˆ°å¤šæ¥è¿‘æ··åˆç²¾åº¦çš„æ•ˆæœï¼Ÿ
- Trade-offçš„ä¸´ç•Œç‚¹åœ¨å“ªé‡Œï¼Ÿ
- å“ªäº›ä»»åŠ¡å¯¹outlieræ›´æ•æ„Ÿï¼Ÿ
```

### 3. ç¡¬ä»¶ååŒè®¾è®¡

```python
ç ”ç©¶é—®é¢˜ï¼š
- å¦‚æœç¡¬ä»¶æ”¯æŒ2-bitçš„residualï¼Œæ˜¯å¦å€¼å¾—ï¼Ÿ
- ç»“æ„åŒ–æ··åˆç²¾åº¦çš„ç¡¬ä»¶æˆæœ¬å¤šå¤§ï¼Ÿ
- åŠ¨æ€scaleçš„ç¡¬ä»¶å¼€é”€å¦‚ä½•ä¼˜åŒ–ï¼Ÿ
```

---

## ğŸ“š å®Œæ•´å‚è€ƒæ–‡çŒ®

1. **LLM.int8()**: Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. "LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale." NeurIPS 2022. https://arxiv.org/abs/2208.07339

2. **SqueezeLLM**: Sehoon Kim, Coleman Hooper, Amir Gholami, Zhen Dong, Xiuyu Li, Sheng Shen, Michael W. Mahoney, and Kurt Keutzer. "SqueezeLLM: Dense-and-Sparse Quantization." ICML 2024. https://arxiv.org/abs/2306.07629

3. **AWQ**: Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, and Song Han. "AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration." MLSys 2024. https://arxiv.org/abs/2306.00978

4. **ATOM**: Yilong Zhao, Chien-Yu Lin, Kan Zhu, Zihao Ye, Lequn Chen, Size Zheng, Luis Ceze, Arvind Krishnamurthy, Tianqi Chen, and Baris Kasikci. "ATOM: Low-bit Quantization for Efficient and Accurate LLM Serving." MLSys 2024.

5. **OWQ**: Changhun Lee, Jungyu Jin, Taesu Kim, Hyungjun Kim, and Eunhyeok Park. "OWQ: Lessons learned from activation outliers for weight quantization in large language models." AAAI 2024. https://github.com/xvyaward/owq

6. **Quamba**: Zheng et al. "Quamba: Efficient State Space Models Through Nested Quantization." (ä½ çš„è®ºæ–‡)

---

**æœ€åæ›´æ–°**: 2025-11-05
**ç»´æŠ¤è€…**: Yizhi Chen
