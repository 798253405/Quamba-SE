# Activation Scale é™æ€/åŠ¨æ€åˆ†æ

**åˆ›å»ºæ—¶é—´**: 2025-11-05
**æ ¸å¿ƒé—®é¢˜**: Activationçš„scaleæ˜¯é™æ€å›ºå®šçš„ï¼Œè¿˜æ˜¯æ¯æ¬¡forwardéƒ½åŠ¨æ€è®¡ç®—ï¼Ÿ

---

## ğŸ¯ ç»“è®ºï¼šå®Œå…¨é™æ€ï¼ˆStaticï¼‰

**Activation scaleæ˜¯åœ¨calibrationæ—¶ä¸€æ¬¡æ€§ç¡®å®šï¼Œä¿å­˜åˆ°æ¨¡å‹ï¼Œruntimeæ—¶ç›´æ¥ä½¿ç”¨å›ºå®šå€¼ã€‚**

---

## ğŸ“Š è¯æ®é“¾ï¼šä»Calibrationåˆ°Runtime

### 1. Calibrationé˜¶æ®µï¼šç”Ÿæˆå›ºå®šScale

**ä½ç½®**: `quamba/modelutils_mamba.py:112-253`

#### 1.1 åˆ›å»ºObserveræ”¶é›†ç»Ÿè®¡

```python
# Line 164-180
observers[i][op + ":input"] = PerTensorPercentileObserver(
    n_bits=8,
    percentile_alpha=0.9995,  # é»˜è®¤å€¼
    sym=True
)
observers[i][op + ":output"] = PerTensorMinmaxObserver(
    n_bits=8,
    sym=True
)
```

**å…³é”®ç‚¹**ï¼š
- æ¯å±‚æ¯ä¸ªopåˆ›å»ºinputå’Œoutputçš„observer
- Observerç”¨äºç´¯ç§¯ç»Ÿè®¡ä¿¡æ¯

#### 1.2 è¿è¡Œ512ä¸ªæ ·æœ¬ï¼Œç´¯ç§¯ç»Ÿè®¡

```python
# Line 205-219
for i in tqdm(range(num_samples)):  # é»˜è®¤512ä¸ªæ ·æœ¬
    input_ids = preprocess_fn(calibration_dataset[i])
    model(input_ids, inference_params=inference_params)
    # â†‘ Observeråœ¨forward hookä¸­è‡ªåŠ¨è°ƒç”¨ update()
```

**Hookå®ç°** (Line 141-149):
```python
def stat_hook(m, inputs, outputs, op, block_idx):
    # æ¯æ¬¡forwardéƒ½æ›´æ–°observerçš„ç»Ÿè®¡
    observers[block_idx][op + ":input"].update(inputs.clone().detach())
    observers[block_idx][op + ":output"].update(outputs.clone().detach())
```

**Observerå†…éƒ¨é€»è¾‘** (`quamba/observer.py:106-154`):
```python
def update(self, w):
    # è®¡ç®—å½“å‰batchçš„maxï¼ˆå¸¦percentileè£å‰ªï¼‰
    cur_max = torch.quantile(w.abs().reshape(-1), self.percentile_alpha)

    # EMAç´¯ç§¯ï¼ˆæŒ‡æ•°ç§»åŠ¨å¹³å‡ï¼‰
    if self.w_max is None:
        self.w_max = cur_max
    else:
        self.w_max = self.w_max + self.percentile_sigma * (cur_max - self.w_max)
```

**å…³é”®ç‚¹**ï¼š
- 512ä¸ªæ ·æœ¬ä¸Šç´¯ç§¯ç»Ÿè®¡ï¼ˆEMAå¹³æ»‘ï¼‰
- æœ€ç»ˆ `self.w_max` æ˜¯ä¸€ä¸ª**å›ºå®šçš„FP32å€¼**

#### 1.3 æå–å›ºå®šScale

```python
# Line 247-253
for i in range(len(layers) + 1):
    for name, observer in observers[i].items():
        scale, base = observer.get_quantization_parameters()  # â† è·å–æœ€ç»ˆå›ºå®šscale
        act_scales[i][name] = scale.to(torch.float32)        # â† è½¬ä¸ºFP32å¹¶ä¿å­˜
del observers
return act_scales  # â† è¿”å›æ‰€æœ‰å›ºå®šçš„scales
```

**`get_quantization_parameters()` å®ç°** (`quamba/observer.py:138-154`):
```python
def get_quantization_parameters(self):
    # ä½¿ç”¨ç´¯ç§¯çš„w_maxè®¡ç®—scale
    scale = self.w_max / 127  # FP32 scalar
    return _get_minmax_quantization_params(
        self.w_min, self.w_max,
        self.n_bits, self.clip_ratio, self.sym
    )
```

**è¿”å›æ ¼å¼**:
```python
act_scales = [
    {  # Layer 0
        "in_proj:input": tensor(0.0234),  # FP32 scalar
        "in_proj:output": tensor(0.0156),
        "x_proj:input": tensor(0.0423),
        "x_proj:output": tensor(0.0389),
        ...
    },
    {  # Layer 1
        ...
    },
    ...
]
```

---

### 2. Model Quantizationé˜¶æ®µï¼šå­˜å‚¨å›ºå®šScale

**ä½ç½®**: `quamba/qMambaLayer.py:811-893`

#### 2.1 å°†å›ºå®šScaleä¼ ç»™QCausalConv1D

```python
# Line 848-852
qmixer.conv1d = QCausalConv1D.from_fp16(
    originalLayer=copy.deepcopy(originalLayer.conv1d),
    input_scale=act_scales["in_proj:output"].item(),   # â† å›ºå®šFP32 scalar
    output_scale=act_scales["x_proj:input"].item(),    # â† å›ºå®šFP32 scalar
)
```

**`QCausalConv1D.from_fp16()` å®ç°** (`quamba/qConvLayer.py:43-73`):
```python
@classmethod
def from_fp16(cls, originalLayer, input_scale=1.0, output_scale=1.0):
    qconv = cls(...)

    # å­˜å‚¨ä¸ºç±»æˆå‘˜å˜é‡ï¼ˆébufferï¼‰
    qconv.input_scale = input_scale    # Line 50: float scalar
    qconv.output_scale = output_scale  # Line 51: float scalar

    # é‡åŒ–weightï¼ˆä¸€æ¬¡æ€§ï¼‰
    int8_weight, weight_scale = quantize_tensor_per_tensor_absmax(...)
    qconv.weight = int8_weight.to(device)
    qconv.weight_scale = weight_scale.item()

    return qconv
```

#### 2.2 Scaleå­˜å‚¨æœºåˆ¶

**åˆå§‹åŒ–æ—¶** (`quamba/qConvLayer.py:35-38`):
```python
self.weight_scale = 0.0
self.bias_scale = 0.0
self.input_scale = 0.0   # â† Activation input scale
self.output_scale = 0.0  # â† Activation output scale
```

**ä¿å­˜åˆ°state_dict** (`quamba/qConvLayer.py:75-90`):
```python
def store_hook(self, module, state_dict, prefix, local_metadata):
    # å°†scaleä¿å­˜åˆ°state_dictï¼ˆæ¨¡å‹æ–‡ä»¶ï¼‰
    state_dict[prefix + 'weight_scale'] = self.weight_scale
    state_dict[prefix + 'bias_scale'] = self.bias_scale
    state_dict[prefix + 'input_scale'] = self.input_scale    # â† ä¿å­˜å›ºå®šå€¼
    state_dict[prefix + 'output_scale'] = self.output_scale  # â† ä¿å­˜å›ºå®šå€¼
    return state_dict

def load_hook(self, state_dict, prefix, ...):
    # ä»state_dictåŠ è½½scale
    self.input_scale = state_dict[prefix + 'input_scale']    # â† åŠ è½½å›ºå®šå€¼
    self.output_scale = state_dict[prefix + 'output_scale']  # â† åŠ è½½å›ºå®šå€¼
    ...
```

**å…³é”®ç‚¹**ï¼š
- Scaleä½œä¸º**ç±»æˆå‘˜å˜é‡**ï¼ˆ`self.input_scale`ï¼‰
- é€šè¿‡hookä¿å­˜/åŠ è½½åˆ°æ¨¡å‹æ–‡ä»¶
- **ä¸€æ—¦è®¾ç½®ï¼Œæ°¸ä¸æ”¹å˜**

---

### 3. Runtime/Inferenceé˜¶æ®µï¼šç›´æ¥ä½¿ç”¨å›ºå®šScale

**ä½ç½®**: `quamba/qConvLayer.py:93-112`

#### 3.1 Forwardä½¿ç”¨å›ºå®šScale

```python
# Line 93-100
@torch.no_grad()
def forward(self, x):
    y = quant_causal_conv1d_cuda.fwd(
        x, self.input_scale,    # â† ç›´æ¥ä½¿ç”¨å›ºå®šå€¼ï¼ˆæ¯æ¬¡forwardéƒ½ç›¸åŒï¼‰
        self.weight, self.weight_scale,
        self.output_scale,      # â† ç›´æ¥ä½¿ç”¨å›ºå®šå€¼ï¼ˆæ¯æ¬¡forwardéƒ½ç›¸åŒï¼‰
        self.bias_scale, self.bias,
        None, None, None, True
    )
    return y
```

**CUDA kernelæ¥æ”¶å›ºå®šScale** (`csrc/causal_conv1d/quamba2_conv1d_fwd_kernel.cuh:171-198`):
```cuda
// Line 171-172: Quamba1 - å•ä¸ªå…¨å±€scale
if (params.x_head_group_range_ptr == nullptr) {
    scale_out = *reinterpret_cast<float *>(params.x_scales_ptr);  // FP32 scalar
}
// Line 173-198: Quamba2 - 128ä¸ªgroup scales
else {
    for (int hg_idx = 0; hg_idx < params.x_nhead_group; hg_idx++) {
        for (int dg_idx = 0; dg_idx < params.x_ndim_group; dg_idx++) {
            scale_out = x_scales[hg_idx * params.x_ndim_group + dg_idx];  // FP32
        }
    }
}

// Line 254-255: é‡åŒ–ä½¿ç”¨å›ºå®šscale
int tmp = int(roundf(out_vals[i] / scale_out));  // â† scale_outæ˜¯å›ºå®šçš„
xBC_smem[...] = tmp > 127 ? 127 : tmp < -128 ? -128 : static_cast<input_t>(tmp);
```

**å…³é”®ç‚¹**ï¼š
- **å®Œå…¨æ²¡æœ‰åŠ¨æ€è®¡ç®—scaleçš„ä»£ç **
- æ¯æ¬¡forwardéƒ½ä½¿ç”¨ç›¸åŒçš„å›ºå®šscale
- å³ä½¿è¾“å…¥ä¸åŒï¼ˆä¸åŒå¥å­ï¼‰ï¼Œscaleä¸å˜

#### 3.2 Quamba2çš„Scaleå­˜å‚¨

**Quamba2ä½¿ç”¨Bufferå­˜å‚¨128ä¸ªScales** (`quamba/qConvLayer.py:183-196`):
```python
# Line 183-189: æ³¨å†Œä¸ºbufferï¼ˆä¼šä¿å­˜åˆ°æ¨¡å‹ï¼‰
if x_nhead_group > 0 and x_ndim_group > 0:
    self.register_buffer('x_out_scales', torch.empty(
        (n_groups, x_nhead_group, x_ndim_group),  # [8, 4, 4] = 128ä¸ªscale
        dtype=torch.float32))  # â† FP32ç²¾åº¦
else:
    self.register_buffer('x_out_scales', torch.empty(
        (1), dtype=torch.float32))  # â† Quamba1: å•ä¸ªscale
```

**è®¾ç½®å›ºå®šå€¼** (`quamba/qConvLayer.py:236-238`):
```python
# from_fp16æ—¶è®¾ç½®
qconv.x_out_scales = x_out_scales.to(device)  # â† ä»calibrationä¼ å…¥çš„å›ºå®štensor
qconv.B_out_scales = B_out_scales.to(device)
qconv.C_out_scales = C_out_scales.to(device)
```

**Forwardç›´æ¥ä¼ å…¥** (`quamba/qConvLayer.py:304-314`):
```python
@torch.no_grad()
def forward(self, xBC):
    x, B, C = quamba2_conv1d_cuda.fwd(
        xBC,
        self.x_scale, self.B_scale, self.C_scale,  # â† å›ºå®šinput scales
        ...
        self.x_out_scales,    # â† å›ºå®šoutput scales (128ä¸ªFP32å€¼)
        self.B_out_scales,
        self.C_out_scales,
        ...
    )
    return x, B, C
```

---

## ğŸ” é™æ€Scaleçš„å·¥ä½œåŸç†

### ä¸ºä»€ä¹ˆå›ºå®šScaleèƒ½ç”¨äºä¸åŒè¾“å…¥ï¼Ÿ

**ç†è®ºåŸºç¡€**ï¼š

1. **LayerNormå½’ä¸€åŒ–** (`quamba/qMambaLayer.py:920-930`):
   ```python
   # æ¯å±‚éƒ½æœ‰RMSNormï¼Œå¼ºåˆ¶æ¿€æ´»å€¼åˆ†å¸ƒå½’ä¸€åŒ–
   x = self.norm(hidden_states)  # RMSNorm
   ```
   - RMSNormä½¿å¾—æ¿€æ´»å€¼åˆ†å¸ƒåœ¨ä¸åŒè¾“å…¥é—´ç›¸å¯¹ç¨³å®š
   - åˆ†å¸ƒæ ‡å‡†å·®åœ¨Â±10%èŒƒå›´å†…æ³¢åŠ¨

2. **ç»Ÿè®¡å¹³ç¨³æ€§**:
   - Calibrationç”¨512ä¸ªæ ·æœ¬ï¼ˆPileæ•°æ®é›†ï¼‰
   - å–EMA of maxï¼Œè¦†ç›–åˆ†å¸ƒçš„95-99%
   - æµ‹è¯•æ—¶ï¼ˆLambadaç­‰ï¼‰çš„åˆ†å¸ƒä¸Pileç›¸è¿‘

3. **ä¿å®ˆä¼°è®¡**:
   ```python
   scale = max / 127  # ä½¿ç”¨maxå€¼ï¼Œä¸æ˜¯mean
   ```
   - å¦‚æœæµ‹è¯•è¾“å…¥çš„max < calibration max â†’ å®Œå…¨OK
   - å¦‚æœæµ‹è¯•è¾“å…¥çš„max > calibration max â†’ é¥±å’Œæˆªæ–­ï¼ˆ0.1-1%æº¢å‡ºï¼‰

4. **ä¼˜é›…é™çº§**:
   ```cuda
   tmp > 127 ? 127 : tmp < -128 ? -128 : tmp  // Clampåˆ°INT8èŒƒå›´
   ```
   - æº¢å‡ºæ—¶é¥±å’Œåˆ°127/-128
   - åªæœ‰å°‘æ•°outlierå—å½±å“ï¼ˆ0.05-1%ï¼‰
   - å®éªŒæ˜¾ç¤ºå¯¹å‡†ç¡®ç‡å½±å“<1%

### ä½•æ—¶Static Scaleä¼šå¤±è´¥ï¼Ÿ

**å¤±è´¥åœºæ™¯**ï¼š
1. **åˆ†å¸ƒåç§»**ï¼šè‹±æ–‡â†’ä¸­æ–‡ï¼Œé¢†åŸŸå·®å¼‚å¤§
2. **æç«¯è¾“å…¥**ï¼šè¶…é•¿æ–‡æ¡£ï¼Œå¼‚å¸¸æ•°æ®
3. **å¾®è°ƒå**ï¼šæ¨¡å‹å‚æ•°æ”¹å˜ï¼Œæ¿€æ´»å€¼åˆ†å¸ƒå˜åŒ–
4. **æ— LayerNorm**ï¼šåˆ†å¸ƒä¸ç¨³å®š

**è§£å†³æ–¹æ¡ˆ**ï¼šé‡æ–°è¿è¡Œcalibration

---

## ğŸ“Š Static vs Dynamicå¯¹æ¯”

| ç‰¹æ€§ | Static Scale (Quamba) | Dynamic Scale |
|------|----------------------|---------------|
| **è®¡ç®—æ—¶æœº** | Calibrationæ—¶ï¼ˆä¸€æ¬¡æ€§ï¼‰ | æ¯æ¬¡forward |
| **Scaleå­˜å‚¨** | æ¨¡å‹å‚æ•°ï¼ˆFP32ï¼‰ | æ— éœ€å­˜å‚¨ |
| **Runtimeå¼€é”€** | 0%ï¼ˆç›´æ¥ä½¿ç”¨ï¼‰ | 5-15%ï¼ˆè®¡ç®—min/max/percentileï¼‰ |
| **ç²¾åº¦** | å›ºå®šï¼ˆè·¨è¾“å…¥ä¸€è‡´ï¼‰ | è‡ªé€‚åº”ï¼ˆæ¯ä¸ªè¾“å…¥æœ€ä¼˜ï¼‰ |
| **é€‚ç”¨åœºæ™¯** | åˆ†å¸ƒç¨³å®šï¼ˆæœ‰LayerNormï¼‰ | åˆ†å¸ƒæ³¢åŠ¨å¤§ |
| **ç¡¬ä»¶å‹å¥½** | âœ… å®Œå…¨é™æ€ï¼Œæ˜“ä¼˜åŒ– | âš ï¸ éœ€è¦é¢å¤–è®¡ç®— |
| **è·¨batchä¸€è‡´æ€§** | âœ… å®Œå…¨ä¸€è‡´ | âŒ æ¯ä¸ªbatchä¸åŒ |

---

## ğŸ’¡ å…³é”®æ´å¯Ÿ

### 1. PTQçš„æœ¬è´¨

```
PTQ (Post-Training Quantization):
  - Weights: é™æ€é‡åŒ–ï¼ˆå›ºå®šscaleï¼‰
  - Activations: é™æ€é‡åŒ–ï¼ˆå›ºå®šscaleï¼ŒQuambaå®ç°ï¼‰

å¯¹æ¯” QAT (Quantization-Aware Training):
  - Weights: å¯å­¦ä¹ scale
  - Activations: å¯å­¦ä¹ scaleæˆ–åŠ¨æ€scale
```

### 2. Quambaçš„é€‰æ‹©

**å®Œå…¨é™æ€é‡åŒ–**ï¼ˆWeights + Activationsï¼‰ï¼š
- âœ… ç¡¬ä»¶å‹å¥½ï¼ˆæ— runtimeè®¡ç®—å¼€é”€ï¼‰
- âœ… æ¨ç†ä¸€è‡´æ€§ï¼ˆåŒæ ·è¾“å…¥æ€»æ˜¯åŒæ ·è¾“å‡ºï¼‰
- âœ… éƒ¨ç½²ç®€å•ï¼ˆæ¨¡å‹è‡ªåŒ…å«scaleï¼‰
- âš ï¸ ä¾èµ–calibrationæ•°æ®è´¨é‡
- âš ï¸ è·¨åŸŸæ³›åŒ–å—é™

### 3. Scaleçš„å±‚æ¬¡ç»“æ„

```
Quamba1ï¼ˆPer-tensorï¼‰:
  Conv1Dæ¯å±‚: 1ä¸ª input_scale + 1ä¸ª output_scale
  Linearæ¯å±‚: 1ä¸ª input_scale + 1ä¸ª output_scale

Quamba2ï¼ˆPiecewiseï¼‰:
  Conv1Dæ¯å±‚: 1ä¸ª input_scale + 128ä¸ª output_scales (FP32 tensor)
  Linearæ¯å±‚: 1ä¸ª input_scale + 1ä¸ª output_scale
```

### 4. ä»£ç ä¸­å®Œå…¨æ²¡æœ‰åŠ¨æ€è®¡ç®—

**éªŒè¯æ–¹å¼**ï¼š
```bash
# æœç´¢æ‰€æœ‰å¯èƒ½çš„åŠ¨æ€scaleè®¡ç®—
grep -r "quantile.*forward" quamba/  # æ— ç»“æœ
grep -r "\.max().*forward" quamba/   # æ— ç»“æœï¼ˆé™¤äº†GPTQé¢„å¤„ç†ï¼‰
grep -r "percentile.*forward" quamba/  # æ— ç»“æœ
```

**ç»“è®º**ï¼šForward pathä¸­**å®Œå…¨æ²¡æœ‰**ç»Ÿè®¡è®¡ç®—ï¼Œåªæœ‰å›ºå®šscaleçš„ä½¿ç”¨ã€‚

---

## ğŸ”§ å®éªŒéªŒè¯

### éªŒè¯1ï¼šæ‰“å°Scaleå€¼

```python
# åœ¨ qConvLayer.py forwardä¸­æ·»åŠ 
def forward(self, x):
    print(f"Input scale: {self.input_scale}")   # æ¯æ¬¡forwardéƒ½ç›¸åŒ
    print(f"Output scale: {self.output_scale}") # æ¯æ¬¡forwardéƒ½ç›¸åŒ
    y = quant_causal_conv1d_cuda.fwd(...)
    return y
```

**é¢„æœŸç»“æœ**ï¼š
```
# Sentence 1
Input scale: 0.042315
Output scale: 0.038912

# Sentence 2 (ä¸åŒè¾“å…¥)
Input scale: 0.042315  # â† å®Œå…¨ç›¸åŒï¼
Output scale: 0.038912  # â† å®Œå…¨ç›¸åŒï¼
```

### éªŒè¯2ï¼šæ£€æŸ¥æ¨¡å‹æ–‡ä»¶

```python
import torch
model = torch.load("quantized_model.pth")

# æŸ¥çœ‹ä¿å­˜çš„scale
for name, param in model.named_parameters():
    if "scale" in name:
        print(f"{name}: {param}")

# é¢„æœŸè¾“å‡ºï¼š
# backbone.layers.0.mixer.conv1d.input_scale: 0.042315
# backbone.layers.0.mixer.conv1d.output_scale: 0.038912
# ...
```

---

## ğŸ“š ç›¸å…³æ–‡ä»¶

### æ ¸å¿ƒå®ç°
- `quamba/observer.py`: Observerç´¯ç§¯ç»Ÿè®¡ï¼Œç”Ÿæˆå›ºå®šscale
- `quamba/qConvLayer.py`: å­˜å‚¨å’Œä½¿ç”¨å›ºå®šscale
- `quamba/modelutils_mamba.py`: Calibrationæµç¨‹ï¼Œç”Ÿæˆact_scales
- `quamba/qMambaLayer.py`: å°†act_scalesè®¾ç½®åˆ°å„å±‚

### CUDAå®ç°
- `csrc/causal_conv1d/quamba2_conv1d_fwd_kernel.cuh`: ä½¿ç”¨å›ºå®šscaleè¿›è¡Œé‡åŒ–

---

## ğŸ¯ æ€»ç»“

**Activation Scaleæ˜¯å®Œå…¨é™æ€çš„ï¼**

1. **Calibrationæ—¶**ï¼ˆä¸€æ¬¡æ€§ï¼‰ï¼š
   - è¿è¡Œ512ä¸ªæ ·æœ¬
   - Observerç´¯ç§¯ç»Ÿè®¡ï¼ˆEMAï¼‰
   - ç”Ÿæˆå›ºå®šçš„FP32 scale

2. **Quantizationæ—¶**ï¼ˆä¸€æ¬¡æ€§ï¼‰ï¼š
   - å°†å›ºå®šscaleè®¾ç½®åˆ°æ¨¡å‹
   - ä¿å­˜åˆ°state_dict

3. **Runtimeæ—¶**ï¼ˆæ¯æ¬¡æ¨ç†ï¼‰ï¼š
   - **ç›´æ¥ä½¿ç”¨å›ºå®šscale**
   - **å®Œå…¨æ²¡æœ‰åŠ¨æ€è®¡ç®—**
   - ä¸åŒè¾“å…¥ç”¨ç›¸åŒscale

**ä¼˜åŠ¿**ï¼š
- é›¶runtimeå¼€é”€
- ç¡¬ä»¶å‹å¥½
- æ¨ç†ä¸€è‡´æ€§

**ä¾èµ–**ï¼š
- LayerNormå½’ä¸€åŒ–ï¼ˆä¿è¯åˆ†å¸ƒç¨³å®šï¼‰
- é«˜è´¨é‡calibrationæ•°æ®ï¼ˆä»£è¡¨æ€§ï¼‰
- ä¿å®ˆçš„scaleé€‰æ‹©ï¼ˆmaxè€Œémeanï¼‰

---

**åˆ›å»ºæ—¶é—´**: 2025-11-05
**éªŒè¯æ–¹å¼**: ä»£ç å®¡æŸ¥ + æ•°æ®æµè¿½è¸ª
**çŠ¶æ€**: âœ… å·²ç¡®è®¤ - Activation scaleæ˜¯é™æ€çš„
