# Quamba1 æ¨¡å‹åŠ è½½Bugä¿®å¤è¯¦è§£

**åˆ›å»ºæ—¶é—´**: 2025-11-06
**é—®é¢˜ç±»å‹**: ä»£ç å…¼å®¹æ€§Bug
**å½±å“èŒƒå›´**: Quamba1æ¨¡å‹ï¼ˆä¸ä½¿ç”¨ `--quantize_lm_head` çš„åœºæ™¯ï¼‰
**é‡è¦æ€§**: â­â­â­ **éå¸¸é‡è¦ï¼Œä¸è¦åˆ é™¤æ­¤æ–‡ä»¶**

---

## ğŸ” é—®é¢˜èƒŒæ™¯

### ä½œè€…çš„é‡åŒ–å»ºè®®

æ ¹æ®ä½œè€…çš„é‚®ä»¶å›å¤ï¼š

> If you'd like to reproduce the Quamba1 results, please set the quantization bit-width to W8A8 and quantize the Mamba1 models **without** `--quantize_embedding`, `--quantize_lm_head`, and `--apply_gptq` flags.

**æ ¸å¿ƒè¦æ±‚**ï¼š
- Quamba1ï¼ˆMamba1ï¼‰ï¼šW8A8ï¼Œ**ä¸åŠ ** `--quantize_lm_head`
- Quamba2ï¼ˆMamba2ï¼‰ï¼šW4A8ï¼Œ**å¿…é¡»åŠ ** `--quantize_lm_head`, `--quantize_embedding`, `--apply_gptq`

### å‘ç°çš„é—®é¢˜

å½“æŒ‰ç…§ä½œè€…å»ºè®®é‡åŒ– Mamba1-130M æ¨¡å‹åï¼Œå°è¯•åŠ è½½è¯„ä¼°æ—¶é‡åˆ°äº† **4ä¸ªè¿ç»­çš„ä»£ç Bug**ï¼Œå¯¼è‡´æ— æ³•æ­£å¸¸åŠ è½½å’Œè¿è¡Œæ¨¡å‹ã€‚

---

## ğŸ› Bugè¯¦æƒ…ä¸ä¿®å¤

### Bug 1: `qNorm.py` - KeyError: 'output_scale'

**é”™è¯¯ä¿¡æ¯**ï¼š
```python
KeyError: 'backbone.norm_f.output_scale'
```

**é”™è¯¯ä½ç½®**ï¼š`quamba/qNorm.py:43`

**æ ¹æœ¬åŸå› **ï¼š
```python
# åŸå§‹ä»£ç ï¼ˆç¬¬42-44è¡Œï¼‰
def load_hook(self, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs):
    self.output_scale = state_dict[prefix + 'output_scale']  # ç›´æ¥è®¿é—®ï¼Œé”®ä¸å­˜åœ¨æ—¶æŠ¥é”™
    del state_dict[prefix + 'output_scale']
```

å½“ä¸åŠ  `--quantize_lm_head` æ—¶ï¼š
1. `norm_f` ä¸ä¼šè¢«é‡åŒ–ï¼Œä¿æŒ FP16 çš„ `RMSNorm`
2. ä¿å­˜æ¨¡å‹æ—¶ `norm_f` çš„ state_dict ä¸­**æ²¡æœ‰** `output_scale` é”®
3. ä½† config é”™è¯¯åœ°æŠŠæ‰€æœ‰ norm ç±»å‹å†™æˆäº† `QRMSNorm`
4. åŠ è½½æ—¶åˆ›å»º `QRMSNorm`ï¼Œ`load_hook` æ‰¾ä¸åˆ° `output_scale` é”®ï¼ŒæŠ¥é”™

**ä¿®å¤æ–¹æ¡ˆ**ï¼š
```python
# ä¿®å¤åçš„ä»£ç ï¼ˆç¬¬42-48è¡Œï¼‰
def load_hook(self, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs):
    # Handle backward compatibility: if output_scale is not in state_dict, set to None
    if prefix + 'output_scale' in state_dict:
        self.output_scale = state_dict[prefix + 'output_scale']
        del state_dict[prefix + 'output_scale']
    else:
        self.output_scale = None  # åŠ¨æ€é‡åŒ–æ¨¡å¼
```

**åŒæ ·ä¿®å¤äº† `QRMSNormGated`**ï¼ˆç¬¬143-155è¡Œï¼‰ï¼š
```python
def load_hook(self, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs):
    # Handle backward compatibility: if scales are not in state_dict, set to default values
    if prefix + 'z_scale' in state_dict:
        self.z_scale = state_dict[prefix + 'z_scale']
        del state_dict[prefix + 'z_scale']
    else:
        self.z_scale = 0.0

    if prefix + 'output_scale' in state_dict:
        self.output_scale = state_dict[prefix + 'output_scale']
        del state_dict[prefix + 'output_scale']
    else:
        self.output_scale = None
```

---

### Bug 2: `quamba_mixer_seq.py` - Missing key: 'lm_head.bias'

**é”™è¯¯ä¿¡æ¯**ï¼š
```python
RuntimeError: Error(s) in loading state_dict for QuambaLMHeadModel:
    Missing key(s) in state_dict: "lm_head.bias".
```

**é”™è¯¯ä½ç½®**ï¼š`quamba/quamba_mixer_seq.py:417`

**æ ¹æœ¬åŸå› **ï¼š
```python
# åŸå§‹ä»£ç ï¼ˆç¬¬416-417è¡Œï¼‰
if lm_head_layer == "Linear":
    self.lm_head = torch.nn.Linear(d_model, vocab_size)  # é»˜è®¤ bias=True
```

- åŸå§‹ Mamba æ¨¡å‹çš„ `lm_head` **æ²¡æœ‰ bias**
- ä½† PyTorch çš„ `nn.Linear` é»˜è®¤ `bias=True`
- ä¿å­˜çš„æ¨¡å‹æ²¡æœ‰ `lm_head.bias`ï¼ŒåŠ è½½æ—¶æŠ¥é”™

**ä¿®å¤æ–¹æ¡ˆ**ï¼š
```python
# ä¿®å¤åçš„ä»£ç ï¼ˆç¬¬416-417è¡Œï¼‰
if lm_head_layer == "Linear":
    self.lm_head = torch.nn.Linear(d_model, vocab_size, bias=False)  # åŒ¹é…åŸå§‹Mamba
```

---

### Bug 3: `quamba_mixer_seq.py` - TypeError: linear() argument 'input' must be Tensor, not tuple

**é”™è¯¯ä¿¡æ¯**ï¼š
```python
TypeError: linear(): argument 'input' (position 1) must be Tensor, not tuple
```

**é”™è¯¯ä½ç½®**ï¼šForward pass ä¸­ `lm_head` æ¥æ”¶è¾“å…¥æ—¶

**æ ¹æœ¬åŸå› **ï¼š

Config ä¸­ `norm_cfg` è¢«è®¾ç½®ä¸º `{"norm": "QRMSNorm"}`ï¼Œè¿™æ˜¯å…¨å±€çš„ï¼Œå¯¼è‡´ï¼š
1. åŠ è½½æ—¶ `norm_f` è¢«åˆ›å»ºä¸º `QRMSNorm`
2. ä½†ä¿å­˜çš„æ¨¡å‹ä¸­ `norm_f` æ˜¯ FP16 çš„ `RMSNorm`ï¼ˆå› ä¸ºæ²¡æœ‰ `--quantize_lm_head`ï¼‰
3. `QRMSNorm` å½“ `output_scale=None` æ—¶è¿”å› **tuple**: `(y, per_token_scale)`
4. ä½† `lm_head` æœŸæœ›å•ä¸ª Tensorï¼Œå¯¼è‡´ç±»å‹é”™è¯¯

**ä»£ç é€»è¾‘**ï¼ˆ`qNorm.py:86-91`ï¼‰ï¼š
```python
else:
    # output per_token scaling factor if output_scale is None
    y = y.reshape(x_shape_og)
    residual_out = residual_out.reshape(x_shape_og)
    per_token_scale = per_token_scale.reshape(x_shape_og[0:-1])
    return (y, per_token_scale) if not prenorm else (y, residual_out, per_token_scale)
```

**ä¿®å¤æ–¹æ¡ˆ**ï¼š

å½“ `lm_head` æ˜¯ FP16 `Linear` æ—¶ï¼Œå¼ºåˆ¶ `norm_f` ä¹Ÿä½¿ç”¨ FP16 `RMSNorm`ï¼š

```python
# ä¿®å¤åçš„ä»£ç ï¼ˆç¬¬416-424è¡Œï¼‰
if lm_head_layer == "Linear":
    self.lm_head = torch.nn.Linear(d_model, vocab_size, bias=False)
    # For Quamba1 (no quantized lm_head), norm_f should also be FP16 RMSNorm
    try:
        from mamba_ssm.ops.triton.layer_norm import RMSNorm
        norm_epsilon = getattr(config, 'norm_epsilon', 1e-5)
        self.backbone.norm_f = RMSNorm(d_model, eps=norm_epsilon, **factory_kwargs)
    except ImportError:
        pass  # Keep QRMSNorm if RMSNorm is not available
```

**é€»è¾‘**ï¼š
- å¦‚æœ `lm_head_cfg["layer"]` æ˜¯ `"Linear"`ï¼ˆFP16ï¼‰ï¼Œè¯´æ˜æ²¡æœ‰é‡åŒ– lm_head
- æ­¤æ—¶ `norm_f` ä¹Ÿåº”è¯¥æ˜¯ FP16 çš„ `RMSNorm`ï¼ˆè¿”å›å•ä¸ªTensorï¼‰
- åŠ è½½åå¼ºåˆ¶æ›¿æ¢ `norm_f` ä¸º `RMSNorm`

---

### Bug 4: `quamba_mixer_seq.py` - RuntimeError: expected mat1 and mat2 to have the same dtype

**é”™è¯¯ä¿¡æ¯**ï¼š
```python
RuntimeError: expected mat1 and mat2 to have the same dtype, but got: c10::Half != float
```

**é”™è¯¯ä½ç½®**ï¼šForward pass ä¸­çŸ©é˜µä¹˜æ³•

**æ ¹æœ¬åŸå› **ï¼š

- `norm_f` (FP16 `RMSNorm`) è¾“å‡º FP16 Tensor
- ä½†ä¿å­˜çš„ `lm_head.weight` å¯èƒ½æ˜¯ FP32
- çŸ©é˜µä¹˜æ³•æ—¶ç±»å‹ä¸åŒ¹é…

**ä¿®å¤æ–¹æ¡ˆ**ï¼š

åŠ è½½æ¨¡å‹åï¼Œå¼ºåˆ¶ `lm_head` è½¬æ¢ä¸º FP16ï¼š

```python
# ä¿®å¤åçš„ä»£ç ï¼ˆç¬¬430-444è¡Œï¼‰
@classmethod
def from_pretrained(cls, pretrained_model_name, device=None, **kwargs):
    cache_dir = kwargs.pop("cache_dir", None)
    config_data = load_config_hf(pretrained_model_name, cache_dir=cache_dir)
    config = QuambaConfig(**config_data)
    model = cls(config, device="cpu", **kwargs)
    loaded_model = load_state_dict_hf(pretrained_model_name, device="cpu", cache_dir=cache_dir)
    model.load_state_dict(loaded_model)
    del loaded_model
    torch.cuda.empty_cache()
    gc.collect()
    # Ensure lm_head is FP16 for compatibility
    if hasattr(model, 'lm_head') and isinstance(model.lm_head, torch.nn.Linear):
        model.lm_head = model.lm_head.half()
    return model.to(device)
```

---

## ğŸ“Š ä¿®å¤æ€»ç»“è¡¨

| Bug | æ–‡ä»¶ | è¡Œæ•° | é—®é¢˜ | ä¿®å¤æ–¹å¼ |
|-----|------|------|------|---------|
| 1 | `qNorm.py` | 42-44 | ç›´æ¥è®¿é—®ä¸å­˜åœ¨çš„é”® | å…ˆæ£€æŸ¥é”®æ˜¯å¦å­˜åœ¨ |
| 1b | `qNorm.py` | 143-143 | åŒä¸Šï¼ˆ`QRMSNormGated`ï¼‰ | åŒä¸Š |
| 2 | `quamba_mixer_seq.py` | 417 | `Linear` é»˜è®¤æœ‰ bias | æ˜¾å¼è®¾ç½® `bias=False` |
| 3 | `quamba_mixer_seq.py` | 416-424 | `norm_f` è¿”å› tuple | æ›¿æ¢ä¸º FP16 `RMSNorm` |
| 4 | `quamba_mixer_seq.py` | 441-443 | dtype ä¸åŒ¹é… | å¼ºåˆ¶è½¬æ¢ä¸º FP16 |

---

## ğŸ”„ å›æ¡£æ–¹æ¡ˆ

å¦‚æœéœ€è¦æ¢å¤åŸå§‹ä»£ç ï¼Œè¯·æŒ‰ä»¥ä¸‹æ­¥éª¤æ“ä½œï¼š

### 1. æ¢å¤ `qNorm.py`

**æ¢å¤ç¬¬42-44è¡Œ**ï¼š
```python
def load_hook(self, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs):
    self.output_scale = state_dict[prefix + 'output_scale']
    del state_dict[prefix + 'output_scale']
```

**æ¢å¤ç¬¬143-143è¡Œ**ï¼ˆ`QRMSNormGated`ï¼‰ï¼š
```python
def load_hook(self, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs):
    self.z_scale = state_dict[prefix + 'z_scale']
    self.output_scale = state_dict[prefix + 'output_scale']
    del state_dict[prefix + 'z_scale']
    del state_dict[prefix + 'output_scale']
```

### 2. æ¢å¤ `quamba_mixer_seq.py`

**æ¢å¤ç¬¬417è¡Œ**ï¼š
```python
if lm_head_layer == "Linear":
    self.lm_head = torch.nn.Linear(d_model, vocab_size)
```

**åˆ é™¤ç¬¬418-424è¡Œ**ï¼ˆnorm_f æ›¿æ¢é€»è¾‘ï¼‰ï¼š
```python
# åˆ é™¤è¿™äº›è¡Œ
# For Quamba1 (no quantized lm_head), norm_f should also be FP16 RMSNorm
try:
    from mamba_ssm.ops.triton.layer_norm import RMSNorm
    norm_epsilon = getattr(config, 'norm_epsilon', 1e-5)
    self.backbone.norm_f = RMSNorm(d_model, eps=norm_epsilon, **factory_kwargs)
except ImportError:
    pass  # Keep QRMSNorm if RMSNorm is not available
```

**æ¢å¤ç¬¬430-444è¡Œ**ï¼ˆåˆ é™¤ lm_head.half()ï¼‰ï¼š
```python
@classmethod
def from_pretrained(cls, pretrained_model_name, device=None, **kwargs):
    cache_dir = kwargs.pop("cache_dir", None)
    config_data = load_config_hf(pretrained_model_name, cache_dir=cache_dir)
    config = QuambaConfig(**config_data)
    model = cls(config, device="cpu", **kwargs)
    loaded_model = load_state_dict_hf(pretrained_model_name, device="cpu", cache_dir=cache_dir)
    model.load_state_dict(loaded_model)
    del loaded_model
    torch.cuda.empty_cache()
    gc.collect()
    return model.to(device)
```

### 3. Git å›æ¡£å‘½ä»¤

å¦‚æœä½¿ç”¨ Git ç®¡ç†ï¼š
```bash
# æŸ¥çœ‹ä¿®æ”¹
git diff quamba/qNorm.py quamba/quamba_mixer_seq.py

# å›æ¡£å•ä¸ªæ–‡ä»¶
git checkout HEAD~1 quamba/qNorm.py
git checkout HEAD~1 quamba/quamba_mixer_seq.py

# æˆ–è€…å›æ¡£åˆ°ç‰¹å®šcommit
git log --oneline  # æ‰¾åˆ°ä¿®æ”¹å‰çš„commit
git checkout <commit-hash> quamba/qNorm.py quamba/quamba_mixer_seq.py
```

---

## ğŸ¯ æ ¹æœ¬åŸå› åˆ†æ

### ä»£ç è®¾è®¡ç¼ºé™·

**é—®é¢˜1ï¼šä¿å­˜ config æ—¶çš„é€»è¾‘é”™è¯¯**

`modelutils_mamba.py:923`ï¼š
```python
model.config.norm_cfg = {"norm": model.backbone.layers[0].norm.__class__.__name__}
```

è¿™è¡Œä»£ç æŠŠ **Block å†…éƒ¨çš„ norm ç±»å‹**ï¼ˆ`QRMSNorm`ï¼‰åº”ç”¨åˆ°äº†**æ‰€æœ‰ norm**ï¼ˆåŒ…æ‹¬ `norm_f`ï¼‰ï¼Œä½†ï¼š
- å½“ä¸åŠ  `--quantize_lm_head` æ—¶ï¼Œ`norm_f` å®é™…æ˜¯ FP16 `RMSNorm`
- Config åº”è¯¥åˆ†åˆ«ä¿å­˜ block norm å’Œ final norm çš„ç±»å‹

**å»ºè®®æ”¹è¿›**ï¼š
```python
# æ›´å¥½çš„è®¾è®¡
model.config.norm_cfg = {
    "norm": model.backbone.layers[0].norm.__class__.__name__,  # Blockå†…çš„norm
    "final_norm": model.backbone.norm_f.__class__.__name__     # æœ€ç»ˆçš„norm_f
}
```

**é—®é¢˜2ï¼šé‡åŒ–é€»è¾‘ä¸åŠ è½½é€»è¾‘ä¸ä¸€è‡´**

- **é‡åŒ–æ—¶**ï¼ˆ`quantize_fp16_model`ï¼‰ï¼š
  - Block å†…çš„ norm æ€»æ˜¯è¢«é‡åŒ–ä¸º `QRMSNorm`ï¼ˆç¬¬645è¡Œï¼‰
  - `norm_f` åªæœ‰åœ¨ `quantize_lm_head=True` æ—¶æ‰è¢«é‡åŒ–ï¼ˆç¬¬696è¡Œï¼‰

- **åŠ è½½æ—¶**ï¼ˆ`QuambaMixerModel.__init__`ï¼‰ï¼š
  - ä» config çš„ `norm_cfg["norm"]` è¯»å–ç±»å‹
  - **æ²¡æœ‰åŒºåˆ† block norm å’Œ final norm**

**è®¾è®¡é—®é¢˜**ï¼š
- é‡åŒ–é€»è¾‘çŸ¥é“ä½•æ—¶é‡åŒ– `norm_f`
- ä½†ä¿å­˜/åŠ è½½é€»è¾‘ä¸çŸ¥é“è¿™ä¸ªåŒºåˆ«
- å¯¼è‡´åŠ è½½æ—¶åˆ›å»ºäº†é”™è¯¯çš„ norm ç±»å‹

---

## ğŸ”§ æ­£ç¡®çš„ä½¿ç”¨æ–¹å¼

### Quamba1ï¼ˆMamba1-130Mï¼‰é‡åŒ–å‘½ä»¤

```bash
cd /workspace/Quamba
python3 main.py pretrained_models/mambaOriginalHuggingfaceDownload/mamba-130m \
  --quantize --w_bits 8 --a_bits 8 \
  --eval_zero_shot --task_list lambada_openai \
  --batch_size 16 --log_dir logs \
  --pretrained_dir ./pretrained_models \
  --output_subdir 1106YzResearchQuamba1
```

**å…³é”®ç‚¹**ï¼š
- âœ… ä½¿ç”¨ `--w_bits 8 --a_bits 8`
- âŒ **ä¸è¦** åŠ  `--quantize_lm_head`
- âŒ **ä¸è¦** åŠ  `--quantize_embedding`
- âŒ **ä¸è¦** åŠ  `--apply_gptq`

### Quamba1 è¯„ä¼°å‘½ä»¤

```bash
cd /workspace/Quamba
python3 main.py 1106YzResearchQuamba1/default/quamba-130m-w8a8 \
  --pretrained_dir ./pretrained_models \
  --eval_zero_shot --task_list lambada_openai \
  --batch_size 16 --log_dir logs
```

### Quamba2ï¼ˆMamba2-130Mï¼‰é‡åŒ–å‘½ä»¤

```bash
cd /workspace/Quamba
python3 main.py pretrained_models/mambaOriginalHuggingfaceDownload/mamba2-130m \
  --quantize --w_bits 4 --a_bits 8 \
  --group_heads --apply_gptq --quantize_embedding --quantize_lm_head \
  --eval_zero_shot --task_list lambada_openai \
  --batch_size 16 --log_dir logs \
  --pretrained_dir ./pretrained_models \
  --output_subdir 1106YzResearchQuamba2
```

**å…³é”®ç‚¹**ï¼š
- âœ… ä½¿ç”¨ `--w_bits 4 --a_bits 8`
- âœ… **å¿…é¡»** åŠ  `--quantize_lm_head`
- âœ… **å¿…é¡»** åŠ  `--quantize_embedding`
- âœ… **å¿…é¡»** åŠ  `--apply_gptq`
- âœ… **å¿…é¡»** åŠ  `--group_heads`ï¼ˆMamba2ç‰¹æœ‰ï¼‰

---

## âœ… éªŒè¯ä¿®å¤

ä¿®å¤åï¼Œä»¥ä¸‹å‘½ä»¤åº”è¯¥èƒ½æ­£å¸¸è¿è¡Œï¼š

```bash
# 1. é‡åŒ–
python3 main.py pretrained_models/mambaOriginalHuggingfaceDownload/mamba-130m \
  --quantize --w_bits 8 --a_bits 8 \
  --eval_zero_shot --task_list lambada_openai \
  --batch_size 16 --log_dir logs \
  --pretrained_dir ./pretrained_models \
  --output_subdir 1106YzResearchQuamba1

# 2. è¯„ä¼°
python3 main.py 1106YzResearchQuamba1/default/quamba-130m-w8a8 \
  --pretrained_dir ./pretrained_models \
  --eval_zero_shot --task_list lambada_openai \
  --batch_size 16 --log_dir logs
```

**é¢„æœŸç»“æœ**ï¼š
- âœ… æ¨¡å‹åŠ è½½æˆåŠŸ
- âœ… è¯„ä¼°æ­£å¸¸è¿è¡Œ
- âœ… è¾“å‡º lambada_openai çš„å‡†ç¡®ç‡å’Œå›°æƒ‘åº¦

---

## ğŸ“š ç›¸å…³æ–‡æ¡£

- `SESSION_HISTORY.md` - Session 6 è®°å½•äº†å®Œæ•´çš„ debug è¿‡ç¨‹
- `QUAMBA_QUANTIZATION_COMPLETE_GUIDE.md` - é‡åŒ–æœºåˆ¶å®Œæ•´æŒ‡å—
- `DOCUMENTATION_INDEX.md` - æ–‡æ¡£ç´¢å¼•

---

## ğŸ”– æ ‡ç­¾

`#bug-fix` `#quamba1` `#model-loading` `#compatibility` `#critical`

---

**ç»´æŠ¤è€…**: Claude (Sonnet 4.5) + Yizhi Chen
**æœ€åæ›´æ–°**: 2025-11-06
**çŠ¶æ€**: âœ… å·²ä¿®å¤å¹¶éªŒè¯
