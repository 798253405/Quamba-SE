# Quamba1 vs Quamba2 æ­£ç¡®å¯¹æ¯”ï¼ˆæ ¹æ®ä½œè€…å›å¤ï¼‰

## ğŸ“§ ä½œè€…å…³é”®å›å¤

> If you'd like to reproduce the **Quamba1** results, please set the quantization bit-width to **W8A8** and quantize the Mamba1 models **without** --quantize_embedding, --quantize_lm_head, and --apply_gptq flags.

---

## ğŸ¯ Quamba1 vs Quamba2 æ ¸å¿ƒåŒºåˆ«

| å¯¹æ¯”é¡¹ | Quamba1 | Quamba2 |
|--------|---------|---------|
| **è®ºæ–‡ç« èŠ‚** | Table 9 | ä¸»è¦ç»“æœ |
| **é€‚ç”¨æ¨¡å‹** | **Mamba1** | **Mamba2** |
| **é‡åŒ–ç²¾åº¦** | **W8A8** | W4A8, W4A16, W8A8 |
| **--quantize** | âœ… | âœ… |
| **--group_heads** | âŒ | âœ… |
| **--apply_gptq** | âŒ | âœ… |
| **--quantize_embedding** | âŒ | âœ… |
| **--quantize_lm_head** | âŒ | âœ… |
| **ä¿å­˜è·¯å¾„** | `quamba1/` | `quamba2/` |
| **é»˜è®¤ percentile** | 0.9995 | 0.99999 |

---

## ğŸ“‹ æ­£ç¡®å‘½ä»¤å¯¹æ¯”

### Quamba1 å‘½ä»¤ï¼ˆMamba1 W8A8ï¼‰

```bash
python3 main.py pretrained_models/mambaOriginalHuggingfaceDownload/mamba-370m \
  --quantize \
  --w_bits 8 \
  --a_bits 8 \
  --batch_size 16 \
  --eval_zero_shot \
  --task_list lambada_openai \
  --pretrained_dir ./pretrained_models \
  --log_dir logs \
  --output_subdir quamba1
  # åªæœ‰è¿™äº›ï¼ä¸åŠ å…¶ä»–ï¼
```

**å…³é”®ç‰¹ç‚¹**:
- âŒ **æ²¡æœ‰** `--group_heads`
- âŒ **æ²¡æœ‰** `--apply_gptq`
- âŒ **æ²¡æœ‰** `--quantize_embedding`
- âŒ **æ²¡æœ‰** `--quantize_lm_head`
- âœ… **ä¿å­˜åˆ°** `quamba1/` æ–‡ä»¶å¤¹

---

### Quamba2 å‘½ä»¤ï¼ˆMamba2 W4A8ï¼‰

```bash
python3 main.py state-spaces/mamba2-2.7b \
  --quantize \
  --group_heads \           # âœ… åŠ è¿™ä¸ª
  --apply_gptq \            # âœ… åŠ è¿™ä¸ª
  --quantize_embedding \    # âœ… åŠ è¿™ä¸ª
  --quantize_lm_head \      # âœ… åŠ è¿™ä¸ª
  --w_bits 4 \
  --a_bits 8 \
  --batch_size 16 \
  --eval_zero_shot \
  --task_list lambada_openai \
  --pretrained_dir ./pretrained_models \
  --log_dir logs \
  --output_subdir quamba2
```

**å…³é”®ç‰¹ç‚¹**:
- âœ… **æœ‰** `--group_heads`
- âœ… **æœ‰** `--apply_gptq`
- âœ… **æœ‰** `--quantize_embedding`
- âœ… **æœ‰** `--quantize_lm_head`
- âœ… **ä¿å­˜åˆ°** `quamba2/` æ–‡ä»¶å¤¹

---

## ğŸ“Š å®éªŒçŸ©é˜µ

### Quamba1 å®éªŒï¼ˆ8ä¸ªï¼‰

| æ¨¡å‹ | é‡åŒ– | percentile_alpha | Paper ç»“æœ |
|------|------|-----------------|-----------|
| Mamba1 130M | W8A8 | é»˜è®¤ (0.9995) | 40.61% |
| Mamba1 130M | W8A8 | **1.0** | - |
| Mamba1 370M | W8A8 | é»˜è®¤ (0.9995) | 50.37% |
| Mamba1 370M | W8A8 | **1.0** | - |
| Mamba1 1.4B | W8A8 | é»˜è®¤ (0.9995) | 60.43% |
| Mamba1 1.4B | W8A8 | **1.0** | - |
| Mamba1 2.8B | W8A8 | é»˜è®¤ (0.9995) | 65.67% |
| Mamba1 2.8B | W8A8 | **1.0** | - |

---

### Quamba2 å®éªŒï¼ˆ12ä¸ªï¼‰

| æ¨¡å‹ | é‡åŒ– | percentile_alpha | Paper ç»“æœ |
|------|------|-----------------|-----------|
| Mamba2 2.7B | W4A8 | é»˜è®¤ (0.99999) | 65.80% |
| Mamba2 2.7B | W4A8 | **1.0** | - |
| Mamba2 2.7B | W4A16 | é»˜è®¤ (0.99999) | 67.50% |
| Mamba2 2.7B | W4A16 | **1.0** | - |
| Mamba2 2.7B | W8A8 | é»˜è®¤ (0.99999) | 68.20% |
| Mamba2 2.7B | W8A8 | **1.0** | - |
| Mamba2 8B | W4A8 | é»˜è®¤ (0.99999) | 69.50% |
| Mamba2 8B | W4A8 | **1.0** | - |
| Mamba2 8B | W4A16 | é»˜è®¤ (0.99999) | 71.20% |
| Mamba2 8B | W4A16 | **1.0** | - |
| Mamba2 8B | W8A8 | é»˜è®¤ (0.99999) | 72.10% |
| Mamba2 8B | W8A8 | **1.0** | - |

**æ€»è®¡**: 20 ä¸ªå®éªŒ

---

## ğŸ“‚ æ–‡ä»¶å¤¹ç»“æ„

```
pretrained_models/
â”œâ”€â”€ quamba1/
â”‚   â”œâ”€â”€ default/              # Quamba1 é»˜è®¤é…ç½®
â”‚   â”‚   â”œâ”€â”€ quamba-130m-w8a8/
â”‚   â”‚   â”œâ”€â”€ quamba-370m-w8a8/
â”‚   â”‚   â”œâ”€â”€ quamba-1.4b-w8a8/
â”‚   â”‚   â””â”€â”€ quamba-2.8b-w8a8/
â”‚   â””â”€â”€ pa-1/                 # Quamba1 pa=1.0
â”‚       â””â”€â”€ (åŒä¸Š4ä¸ªæ¨¡å‹)
â”‚
â””â”€â”€ quamba2/
    â”œâ”€â”€ default/              # Quamba2 é»˜è®¤é…ç½®
    â”‚   â”œâ”€â”€ quamba2-2.7b-w4a8/
    â”‚   â”œâ”€â”€ quamba2-2.7b-w4a16/
    â”‚   â”œâ”€â”€ quamba2-2.7b-w8a8/
    â”‚   â”œâ”€â”€ quamba2-8b-converted-w4a8/
    â”‚   â”œâ”€â”€ quamba2-8b-converted-w4a16/
    â”‚   â””â”€â”€ quamba2-8b-converted-w8a8/
    â””â”€â”€ pa-1/                 # Quamba2 pa=1.0
        â””â”€â”€ (åŒä¸Š6ä¸ªæ¨¡å‹)
```

---

## âœ… è„šæœ¬éªŒè¯

### è„šæœ¬åç§°
`run_correct_experiments.sh`

### éªŒè¯è¦ç‚¹

**Quamba1 éƒ¨åˆ†**:
```bash
grep -A 10 "Quamba1: Mamba1 370M W8A8 - é»˜è®¤" run_correct_experiments.sh
```

åº”è¯¥çœ‹åˆ°ï¼š
```bash
python3 main.py .../mamba-370m \
  --quantize \
  --w_bits 8 \
  --a_bits 8 \
  ... (æ²¡æœ‰ gptq/embedding/lm_head)
  --output_subdir quamba1  # â† å…³é”®ï¼
```

**Quamba2 éƒ¨åˆ†**:
```bash
grep -A 15 "Quamba2: Mamba2 2.7B W4A8 - é»˜è®¤" run_correct_experiments.sh
```

åº”è¯¥çœ‹åˆ°ï¼š
```bash
python3 main.py state-spaces/mamba2-2.7b \
  --quantize \
  --group_heads \           # â† æœ‰è¿™ä¸ª
  --apply_gptq \            # â† æœ‰è¿™ä¸ª
  --quantize_embedding \    # â† æœ‰è¿™ä¸ª
  --quantize_lm_head \      # â† æœ‰è¿™ä¸ª
  --w_bits 4 \
  --a_bits 8 \
  ...
  --output_subdir quamba2  # â† å…³é”®ï¼
```

---

## ğŸš€ å¯åŠ¨å‘½ä»¤

```bash
cd /home/yz/myprojects/2025/logquamba/Quamba
nohup ./run_correct_experiments.sh > experiments_correct.log 2>&1 &
```

---

## ğŸ¯ é¢„æœŸæˆæœ

### Quamba1 (Table 9 å¤ç°)
- âœ… éªŒè¯è®ºæ–‡ Table 9 çš„ Mamba1 W8A8 ç»“æœ
- âœ… ç ”ç©¶ percentile_alpha å¯¹ Quamba1 çš„å½±å“

### Quamba2 (ä¸»è¦ç»“æœå¤ç°)
- âœ… éªŒè¯è®ºæ–‡ä¸»è¦çš„ Mamba2 é‡åŒ–ç»“æœ
- âœ… å¯¹æ¯” W4A8 vs W4A16 vs W8A8
- âœ… ç ”ç©¶ percentile_alpha å¯¹ Quamba2 çš„å½±å“

### æ–¹æ³•å¯¹æ¯”
- âœ… ç†è§£ embedding/lm_head/gptq å¯¹å‡†ç¡®ç‡çš„è´¡çŒ®
- âœ… åˆ†æ Quamba1 å’Œ Quamba2 çš„æŠ€æœ¯å·®å¼‚

---

**æ€»ç»“**: ç°åœ¨çš„è„šæœ¬å®Œå…¨æŒ‰ç…§ä½œè€…çš„æŒ‡å¯¼ä¿®æ­£ï¼Œç¡®ä¿æ­£ç¡®å¤ç°è®ºæ–‡ç»“æœï¼
