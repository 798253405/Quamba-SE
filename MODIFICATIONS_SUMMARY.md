# ğŸ¯ Quamba ä¿®æ”¹æ€»ç»“

**æ—¥æœŸ**: 2025-11-10
**ç‰ˆæœ¬**: v2.0

---

## âœ… å®Œæˆçš„ä¿®æ”¹

### 1. **ä¿®å¤ Mode 2-4 çš„ SSM è°ƒç”¨**

**æ–‡ä»¶**: `quamba/qSelectiveScan.py:302`

**ä¿®æ”¹å‰**:
```python
return execute_fp32_modes('fp32_upper_bound', ...)
```

**ä¿®æ”¹å**:
```python
return execute_fp32_modes('mode22_fp32_replicates_mode21', ...)
```

**åŸå› **: Mode 2-4 åº”è¯¥ä½¿ç”¨ Mode 2-2 åŒæ¬¾ FP32 SSMï¼Œè€Œä¸æ˜¯ fp32_upper_bound

---

### 2. **ä¿®å¤ Mode 2-4 çš„ Requantization**

**æ–‡ä»¶**: `quamba/qMambaLayer.py:676-681`

**ä¿®æ”¹å‰**:
```python
fp32_mode_enabled = (
    os.environ.get('FP32_SSM_INPUT', 'false').lower() == 'true' or
    os.environ.get('FLOAT_SIM_ASIC_INT8', 'false').lower() == 'true' or
    os.environ.get('FLOAT_SIM_ASIC_RESEARCH_SE', 'false').lower() == 'true'
)
```

**ä¿®æ”¹å**:
```python
fp32_mode_enabled = (
    os.environ.get('FLOAT_SIM_ASIC_INT8', 'false').lower() == 'true' or
    os.environ.get('FLOAT_SIM_ASIC_RESEARCH_SE', 'false').lower() == 'true' or
    os.environ.get('CONV1D_MODE24_FP32', 'false').lower() == 'true'
)
```

**åŸå› **: è®© Mode 2-4 ä¹Ÿèµ° fp32_mode_enabled è·¯å¾„ï¼Œç¡®ä¿ x è¢« requantize ä¸º INT8 ç”¨äº x_proj

---

### 3. **åˆ é™¤ Mode 1 (FP32_SSM_INPUT)**

**ä¿®æ”¹æ–‡ä»¶**:
- `quamba/qSelectiveScan.py`: åˆ é™¤æ‰€æœ‰ `FP32_SSM_INPUT` å¼•ç”¨
- `quamba/qMambaLayer.py`: ä» `fp32_mode_enabled` ä¸­åˆ é™¤ `FP32_SSM_INPUT`

**åŸå› **: Mode 1 çš„ dt/B/C ä¹Ÿæ˜¯ INT8ï¼Œä¸æ˜¯å®Œå…¨ FP32ï¼Œä¸”ä¸ Mode 3 å†—ä½™

---

### 4. **å¢å¼º Layer 24 æ‰“å°ä¿¡æ¯**

**æ–‡ä»¶**:
- `quamba/qConvLayer.py`: Lines 200-222 (Mode 2-0/2-2)
- `quamba/qConvLayer.py`: Lines 332-354 (Mode 2-3)
- `quamba/qConvLayer.py`: Lines 385-407 (Mode 2-4)
- `quamba/qMambaLayer.py`: Lines 775-799 (SSM scales)

**æ–°å¢æ‰“å°å†…å®¹**:

#### Conv1D è¾“å‡º (æ‰€æœ‰æ¨¡å¼)
```
================================================================================
[Layer 24 / Counter 23] Conv1D Output (Mode X)
================================================================================
  Location: qConvLayer.py forward() - Mode X path
  Conv1D Kernel: ...

  Output:
    dtype: ...
    shape: ...
    range: ...
    absmax: ...

  Scales:
    input_scale  = ...  (used by Conv1D CUDA kernel for input)
    output_scale = ...  (ç”¨é€”è¯´æ˜)

  Next Step:
    â†’ qMambaLayer.py: ...
    â†’ x_proj: ...
    â†’ qSelectiveScan.py: ...
================================================================================
```

#### SSM Scales (fp32_mode_enabled è·¯å¾„)
```
================================================================================
[Layer 24 / layer_idx 23] SSM Scales
================================================================================
  Location: qMambaLayer.py forward() - fp32_mode_enabled branch
  SSM input (u) dtype: ...
  dt/B/C dtype: ...

  SSM Scales (from self.selective_scan / QSScan):
    u_scale          = ...  (for SSM input u)
    dt_scale         = ...  (for dt)
    B_scale          = ...  (for B)
    C_scale          = ...  (for C)
    A_scale          = ...  (for A)
    D_scale          = ...  (for D)
    z_scale          = ...  (for z)
    ssm_state_scale  = ...  (for state)
    dt_bias_scale    = ...  (for dt_bias)

  âš ï¸  Important: Conv1D output_scale should match SSM u_scale
    (Conv1D output_scale printed above)
    SSM u_scale = ...
================================================================================
```

---

### 5. **åˆ›å»ºæ¨¡å¼é…ç½®ç³»ç»Ÿ**

**æ–°å¢æ–‡ä»¶**: `quamba/mode_config.py`

**åŠŸèƒ½**:
- ä½¿ç”¨ `QUAMBA_MODE` å•ä¸€ç¯å¢ƒå˜é‡
- è‡ªåŠ¨è®¾ç½®æ‰€æœ‰ç›¸å…³çš„ç¯å¢ƒå˜é‡
- æ¨¡å¼éªŒè¯å’Œä¿¡æ¯æ‰“å°

**ä½¿ç”¨æ–¹æ³•**:
```python
from quamba.mode_config import setup_quamba_mode

setup_quamba_mode('2-4')  # è‡ªåŠ¨è®¾ç½®ç¯å¢ƒå˜é‡
```

æˆ–ï¼š
```bash
QUAMBA_MODE=2-4 python3 main.py ...
```

---

### 6. **åˆ›å»ºè¿è¡Œå‘½ä»¤æ–‡æ¡£**

**æ–°å¢æ–‡ä»¶**:
- `RUN_MODES.md`: æ‰€æœ‰æ¨¡å¼çš„è¯¦ç»†è¿è¡Œå‘½ä»¤
- `MODE_CONFIG_USAGE.md`: æ¨¡å¼é…ç½®ä½¿ç”¨è¯´æ˜
- `QUICK_RUN.sh`: å¿«é€Ÿè¿è¡Œè„šæœ¬

**ä½¿ç”¨**:
```bash
# è¿è¡Œå•ä¸ªæ¨¡å¼
./QUICK_RUN.sh 2-4

# è¿è¡Œæ‰€æœ‰æ¨¡å¼
./QUICK_RUN.sh all
```

---

## ğŸ”§ ä¿®å¤çš„é—®é¢˜

### é—®é¢˜ 1: Mode 2-4 SSM é”™è¯¯
- **ç—‡çŠ¶**: Mode 2-4 è°ƒç”¨äº†é”™è¯¯çš„ SSM (`fp32_upper_bound`)
- **åŸå› **: ä»£ç ä¸­ç¡¬ç¼–ç äº†é”™è¯¯çš„æ¨¡å¼å­—ç¬¦ä¸²
- **ä¿®å¤**: æ”¹ä¸º `mode22_fp32_replicates_mode21`

### é—®é¢˜ 2: Mode 2-4 Requantization ç¼ºå¤±
- **ç—‡çŠ¶**: Mode 2-4 æ²¡æœ‰èµ° requantization è·¯å¾„ï¼Œdt/B/C å¯èƒ½ä¸æ˜¯ INT8
- **åŸå› **: `CONV1D_MODE24_FP32` æ²¡æœ‰åŠ å…¥ `fp32_mode_enabled` æ£€æŸ¥
- **ä¿®å¤**: æ·»åŠ åˆ°æ¡ä»¶åˆ¤æ–­ä¸­

### é—®é¢˜ 3: Import é”™è¯¯
- **ç—‡çŠ¶**: `ModuleNotFoundError: No module named 'quamba.SoftEdgeSSM'`
- **åŸå› **: æ–‡ä»¶è¢«è¯¯åˆ é™¤
- **ä¿®å¤**: æ¢å¤æ–‡ä»¶ï¼Œç¡®ä¿ import æ­£ç¡®

### é—®é¢˜ 4: temp-originalquamba å‘åå…¼å®¹æ€§

#### 4.1 RMSNorm Scales ç¼ºå¤±å’Œ dtype ä¸åŒ¹é…
- **ç—‡çŠ¶**:
  - `KeyError: 'backbone.norm_f.output_scale'` åŠ è½½æ—§æ¨¡å‹æ—¶
  - `RuntimeError: mat1 and mat2 must have the same dtype, but got Char and Float` åœ¨ lm_head forward æ—¶
- **åŸå› **:
  - æ—§ç‰ˆæœ¬æ¨¡å‹çš„ state_dict ä¸­æ²¡æœ‰ä¿å­˜ output_scale/z_scale
  - ä½¿ç”¨é»˜è®¤å€¼ 0.0 å¯¼è‡´ norm_f è¾“å‡º INT8 (Char)ï¼Œä½† lm_head æœŸæœ› FP32 (Float)
  - 0.0 æ˜¯æ— æ•ˆçš„é‡åŒ– scaleï¼Œå¯¼è‡´ INT8 è¾“å‡ºæ— æ³•æ­£ç¡®ä½¿ç”¨
- **ä¿®å¤**:
  1. ä¿®æ”¹ load_hookï¼Œå½“ key ç¼ºå¤±æ—¶è®¾ç½® `output_scale = None`
  2. ä¿®æ”¹ forwardï¼Œå½“ `output_scale is None` æ—¶è‡ªåŠ¨ dequantize åˆ° FP32
- **æ•ˆæœ**: æ—§æ¨¡å‹åŠ è½½å norm_f è¾“å‡º FP32ï¼Œä¸æ ‡å‡† lm_head å…¼å®¹

#### 4.2 LM Head ç¼ºå¤± Keys
- **ç—‡çŠ¶**: `RuntimeError: Missing key(s) in state_dict: "lm_head.bias"`
- **åŸå› **: æ—§æ¨¡å‹ä½¿ç”¨ `torch.nn.Linear` lm_headï¼Œæ–°æ¨¡å‹å¯èƒ½ä½¿ç”¨é‡åŒ– lm_headï¼Œå‚æ•°ç»“æ„ä¸åŒ
- **ä¿®å¤**: åœ¨ `from_pretrained` ä¸­ä½¿ç”¨ `strict=False` åŠ è½½ state_dictï¼Œå…è®¸ç¼ºå¤±å’Œé¢å¤–çš„ keys
- **æ•ˆæœ**: ç¼ºå¤±çš„å‚æ•°ä¼šä½¿ç”¨æ¨¡å‹åˆå§‹åŒ–çš„é»˜è®¤å€¼

---

## ğŸ“‹ æœ€ç»ˆæ¨¡å¼å®šä¹‰

| Mode | Conv1D è¾“å‡º | SSM | ç¯å¢ƒå˜é‡ |
|------|------------|-----|---------|
| **0** | INT8 | CUDA INT8 | (é»˜è®¤) |
| **2-0** | FP32 (INT8 grid) | CUDA INT8 (requant) | `FLOAT_SIM_ASIC_INT8=true SSM_USE_CUDA_FOR_FP32=true` |
| **2-1** | INT8 | PyTorch INT8 | `FLOAT_SIM_ASIC_INT8=true SSM_USE_PYTORCH_INT8=true` |
| **2-2** | FP32 (INT8 grid) | Mode 2-2 FP32 | `FLOAT_SIM_ASIC_INT8=true` |
| **2-3** | FP32 (TRUE) | PyTorch INT8 (requant) | `FLOAT_SIM_ASIC_INT8=true CONV1D_MODE23_FP32=true` |
| **2-4** âœ… | FP32 (TRUE) | Mode 2-2 FP32 âœ… | `FLOAT_SIM_ASIC_INT8=true CONV1D_MODE24_FP32=true` |
| **3** | FP32 (TRUE, åŠ¨æ€é‡åŒ–) | FP32 (`selective_scan_SE_float`) | `CONV1D_MODE3_FP32=true` |
| ~~**1**~~ | ~~åˆ é™¤~~ | ~~åˆ é™¤~~ | ~~åˆ é™¤~~ |

---

## ğŸš€ å¿«é€Ÿè¿è¡Œå‘½ä»¤

### Mode 2-4 (ä¿®å¤å)
```bash
FLOAT_SIM_ASIC_INT8=true CONV1D_MODE24_FP32=true \
python3 main.py quamba-130m-w8a8 \
    --pretrained_dir pretrained_models/testPercentileRange/pa-1 \
    --quantize --float-sim-asic-int8 \
    --eval_zero_shot --task_list lambada_openai --testing \
    --log_dir logs_mode24
```

### æ‰€æœ‰æ¨¡å¼
```bash
./QUICK_RUN.sh all
```

---

### 7. **ä¿®å¤ SSM Scales é‡å¤æ‰“å°**

**æ–‡ä»¶**: `quamba/qMambaLayer.py:775-802, 1184-1213`

**é—®é¢˜**: Layer 24 SSM scales åœ¨æ¯æ¬¡ forward pass æ—¶éƒ½æ‰“å°ï¼Œå¯¼è‡´é‡å¤å¤šæ¬¡

**ä¿®å¤**:
```python
# æ·»åŠ æ‰“å°æ ‡å¿—ï¼Œåªæ‰“å°ä¸€æ¬¡
if self.layer_idx == 23:
    if not hasattr(self, '_ssm_scales_printed'):
        self._ssm_scales_printed = False

    if not self._ssm_scales_printed:
        # ... æ‰“å°ä»£ç  ...
        self._ssm_scales_printed = True
```

**åŸå› **: forward() æ–¹æ³•åœ¨æ¨ç†æ—¶æ¯ä¸ªæ ·æœ¬éƒ½ä¼šè°ƒç”¨ï¼Œéœ€è¦é™åˆ¶æ‰“å°æ¬¡æ•°

---

### 8. **ä¿®å¤ Conv1D è¾“å‡ºç±»å‹å’Œ SSM è¾“å…¥è·¯å¾„**

**é—®é¢˜**: Mode 2-0/2-1/2-2 çš„ Conv1D é”™è¯¯åœ°è¿”å› FP32ï¼Œå¯¼è‡´ Mode 2-1 æ— æ³•èµ°æ­£ç¡®çš„ INT8 SSM è·¯å¾„

**ä¿®æ”¹æ–‡ä»¶**:
- `quamba/qConvLayer.py:152-224`
- `quamba/qMambaLayer.py:676-679, 742-763`

**ä¿®æ”¹å‰**:
```python
# qConvLayer.py
else:  # float_sim_asic_int8 == True
    y_fp32_quantized = y_int8.float() * self.output_scale
    return y_fp32_quantized  # âŒ è¿”å› FP32

# qMambaLayer.py
fp32_mode_enabled = (
    os.environ.get('FLOAT_SIM_ASIC_INT8', 'false').lower() == 'true' or
    os.environ.get('CONV1D_MODE24_FP32', 'false').lower() == 'true'
)  # âŒ Mode 3 æ²¡æœ‰åŒ…å«
```

**ä¿®æ”¹å**:
```python
# qConvLayer.py
else:  # float_sim_asic_int8 == True
    return y  # âœ… è¿”å› INT8 (same as Mode 0)

# qMambaLayer.py
fp32_mode_enabled = (
    os.environ.get('FLOAT_SIM_ASIC_INT8', 'false').lower() == 'true' or
    os.environ.get('CONV1D_MODE24_FP32', 'false').lower() == 'true' or
    os.environ.get('CONV1D_MODE3_FP32', 'false').lower() == 'true'  # âœ… åŠ å…¥ Mode 3
)

# æ ¹æ®æ¨¡å¼å†³å®šæ˜¯å¦ dequantize
if ssm_use_pytorch_int8 and not conv1d_mode23_fp32:
    x_for_ssm = x  # Mode 2-1: ä¿æŒ INT8
else:
    x_for_ssm = x.float() * self.conv1d.output_scale  # å…¶ä»–æ¨¡å¼: dequantize
```

**åŸå› **: æ ¹æ®ç”¨æˆ·æè¿°çš„æ­£ç¡®è·¯å¾„ï¼ŒMode 2-0/2-1/2-2 çš„ Conv1D åº”è¯¥è¾“å‡º INT8ï¼Œdequantization åœ¨ qMambaLayer ä¸­æ ¹æ®å…·ä½“æ¨¡å¼å†³å®š

**ä¿®å¤åçš„æ­£ç¡®è·¯å¾„**:

| Mode | Conv1D è¾“å‡º | qMambaLayer å¤„ç† | SSM è¾“å…¥ |
|------|-----------|----------------|---------|
| **0** | INT8 | ä¸å˜ | INT8 â†’ CUDA INT8 SSM |
| **2-0** | INT8 | dequantize â†’ FP32 | FP32 â†’ requantize â†’ CUDA INT8 SSM |
| **2-1** | INT8 | ä¿æŒ INT8 âœ… | INT8 â†’ PyTorch INT8 SSM âœ… |
| **2-2** | INT8 | dequantize â†’ FP32 | FP32 â†’ PyTorch FP32 SSM |
| **2-3** | FP32 | requantize â†’ INT8 | FP32 â†’ requantize â†’ PyTorch INT8 SSM |
| **2-4** | FP32 | requantize â†’ INT8 | FP32 â†’ PyTorch FP32 SSM |
| **3** | FP32 | requantize â†’ INT8 | FP32 â†’ PyTorch FP32 SSM |

---

### 9. **ä¿®å¤ temp-originalquamba å‘åå…¼å®¹æ€§**

#### 9.1 RMSNorm Scales ç¼ºå¤±

**æ–‡ä»¶**: `temp-originalquamba/quamba/qNorm.py:42-47, 142-152`

**é—®é¢˜**: åŠ è½½æ—§ç‰ˆæœ¬é¢„è®­ç»ƒæ¨¡å‹æ—¶å‡ºç° `KeyError: 'backbone.norm_f.output_scale'`ï¼Œå› ä¸ºæ—§æ¨¡å‹çš„ state_dict ä¸­æ²¡æœ‰ output_scale/z_scale å­—æ®µ

**ä¿®å¤**:
```python
# QRMSNorm.load_hook (lines 42-49):
def load_hook(self, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs):
    # Handle backward compatibility: if output_scale is not in state_dict, set to None for FP32 output
    if prefix + 'output_scale' in state_dict:
        self.output_scale = state_dict[prefix + 'output_scale']
        del state_dict[prefix + 'output_scale']
    else:
        # Old checkpoint without output_scale: use None for FP32 output (no quantization)
        self.output_scale = None

# QRMSNormGated.load_hook (lines 149-163):
def load_hook(self, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs):
    # Handle backward compatibility: if scales are not in state_dict, set to None for FP32 output
    if prefix + 'z_scale' in state_dict:
        self.z_scale = state_dict[prefix + 'z_scale']
        del state_dict[prefix + 'z_scale']
    else:
        pass  # keep self.z_scale from __init__ (0.0)

    if prefix + 'output_scale' in state_dict:
        self.output_scale = state_dict[prefix + 'output_scale']
        del state_dict[prefix + 'output_scale']
    else:
        # Old checkpoint without output_scale: use None for FP32 output (no quantization)
        self.output_scale = None

# QRMSNorm.forward æ–°å¢è‡ªåŠ¨ dequantization (lines 83-97):
if self.output_scale is not None:
    # Static quantization: return INT8 output
    y = y.reshape(x_shape_og)
    residual_out = residual_out.reshape(x_shape_og)
    return y if not prenorm else (y, residual_out)
else:
    # Dynamic per-token quantization: dequantize to FP32 for compatibility
    # This is used for old checkpoints without quantized lm_head
    y = y.reshape(x_shape_og)
    residual_out = residual_out.reshape(x_shape_og)
    per_token_scale = per_token_scale.reshape(x_shape_og[0:-1])
    # Dequantize: y is INT8, convert to FP32 using per-token scales
    y_fp32 = y.float() * per_token_scale.unsqueeze(-1)
    residual_fp32 = residual_out.float() if residual_out.dtype == torch.int8 else residual_out
    return y_fp32 if not prenorm else (y_fp32, residual_fp32)
```

**é‡è¦è®¾è®¡å†³ç­–**:
- âœ… **è®¾ç½® output_scale = None**: è§¦å‘è‡ªåŠ¨ FP32 dequantization
  - `output_scale is not None`: è¿”å› INT8 (ç”¨äºé‡åŒ–æ¨¡å‹)
  - `output_scale is None`: è¿”å› FP32 (ç”¨äºæ—§ç‰ˆæœ¬éé‡åŒ–æ¨¡å‹) â­
- âœ… **forward ä¸­è‡ªåŠ¨ dequantize**: ç»´æŒè¿”å›å€¼ç±»å‹ä¸€è‡´æ€§ (å§‹ç»ˆè¿”å›å•ä¸ª tensor)
- âŒ **ä¸èƒ½ç”¨ 0.0 ä½œä¸ºé»˜è®¤å€¼**: 0.0 æ˜¯æ— æ•ˆçš„é‡åŒ– scaleï¼Œä¼šå¯¼è‡´ INT8 è¾“å‡ºä½†æ— æ³•æ­£ç¡® dequantize

**åŸå› **:
- æ—§ç‰ˆæœ¬æ¨¡å‹åœ¨è®­ç»ƒæ—¶æ²¡æœ‰ä¿å­˜ output_scale/z_scale åˆ° state_dict
- æ—§æ¨¡å‹çš„ norm_f åº”è¯¥è¾“å‡º FP32 (å› ä¸º lm_head æ˜¯æ ‡å‡† torch.nn.Linear)
- è®¾ç½® output_scale=None å¹¶åœ¨ forward ä¸­è‡ªåŠ¨ dequantize ä¿è¯è¾“å‡ºæ˜¯ FP32
- è¿™æ ·å¯ä»¥ä¸æ ‡å‡† lm_head å…¼å®¹ï¼Œé¿å… dtype ä¸åŒ¹é…é”™è¯¯

**ä½œè€…å»ºè®®** (å‚è€ƒä¿¡æ¯):
> "This is expected. If you do not use --quantize_lm_head, then there will be KeyError: 'backbone.norm_f.output_scale'. This is because that in this case, we do not save backbone.norm_f.output_scale in checkpoint. I suggest that you can patch the loader to not use w8a8 for embedding and lm_head when load ckpt in the original repo."

#### 9.2 LM Head ç¼ºå¤± Keys (lm_head.bias)

**æ–‡ä»¶**: `temp-originalquamba/quamba/quamba_mixer_seq.py:430-435`

**é—®é¢˜**: åŠ è½½æ²¡æœ‰ä½¿ç”¨ `--quantize_lm_head` è®­ç»ƒçš„æ¨¡å‹æ—¶å‡ºç° `RuntimeError: Missing key(s) in state_dict: "lm_head.bias"`

**åŸå› **:
- æ—§æ¨¡å‹ä½¿ç”¨æ ‡å‡† `torch.nn.Linear` lm_head (æœ‰ weight å’Œ bias)
- æ–°ä»£ç å¯èƒ½ä½¿ç”¨é‡åŒ– lm_head (W8A8B16O16Linear ç­‰)ï¼Œç»“æ„ä¸åŒ
- é‡åŒ–å±‚å¯èƒ½æ²¡æœ‰ bias æˆ–ä½¿ç”¨ä¸åŒçš„å‚æ•°åç§°

**ä¿®å¤**:
```python
# ä¿®æ”¹å‰:
model.load_state_dict(loaded_model)

# ä¿®æ”¹å:
# Use strict=False to allow missing keys (e.g., when checkpoint was saved without --quantize_lm_head)
missing_keys, unexpected_keys = model.load_state_dict(loaded_model, strict=False)
if missing_keys:
    print(f"Warning: Missing keys in state_dict: {missing_keys}")
if unexpected_keys:
    print(f"Warning: Unexpected keys in state_dict: {unexpected_keys}")
```

**æ•ˆæœ**:
- å…è®¸ç¼ºå¤±çš„ keys (å¦‚ `lm_head.bias`)ï¼Œæ¨¡å‹ä¼šä½¿ç”¨åˆå§‹åŒ–çš„é»˜è®¤å€¼
- æ‰“å°è­¦å‘Šä¿¡æ¯ä»¥æé†’ç”¨æˆ· state_dict ä¸å®Œå…¨åŒ¹é…
- ç¬¦åˆä½œè€…å»ºè®®çš„å‘åå…¼å®¹æ€§ä¿®å¤

---

## âœ… éªŒè¯æ¸…å•

### ä¸»è¦åŠŸèƒ½ä¿®å¤
- [x] Mode 2-4 SSM è°ƒç”¨ä¿®å¤
- [x] Mode 2-4 requantization ä¿®å¤
- [x] Mode 1 åˆ é™¤
- [x] Conv1D è¾“å‡ºç±»å‹ä¿®å¤ (Mode 2-0/2-1/2-2 è¿”å› INT8)
- [x] Mode 2-1 SSM è·¯å¾„ä¿®å¤ (ä¿æŒ INT8 è¾“å…¥)
- [x] Mode 3 åŠ å…¥ fp32_mode_enabled
- [x] FLOAT_SIM_ASIC_RESEARCH_SE åˆ é™¤

### è°ƒè¯•å’Œå·¥å…·
- [x] Layer 24 æ‰“å°å¢å¼º (Conv1D + SSM scales)
- [x] SSM scales é‡å¤æ‰“å°ä¿®å¤
- [x] æ¨¡å¼é…ç½®ç³»ç»Ÿåˆ›å»º
- [x] è¿è¡Œå‘½ä»¤æ–‡æ¡£åˆ›å»º

### å‘åå…¼å®¹æ€§ä¿®å¤
- [x] Import é”™è¯¯ä¿®å¤ (SoftEdgeSSM)
- [x] temp-originalquamba RMSNorm scales ç¼ºå¤±å¤„ç†
- [x] temp-originalquamba lm_head.bias ç¼ºå¤±å¤„ç† (strict=False)

---

**çŠ¶æ€**: âœ… **æ‰€æœ‰ä¿®æ”¹å®Œæˆï¼ŒåŒ…æ‹¬å‘åå…¼å®¹æ€§ä¿®å¤ï¼**

**å¯ä»¥è¿è¡Œæµ‹è¯•**:
- Mode 0 å’Œ Mode 2-0 ç²¾åº¦é—®é¢˜åº”è¯¥å·²è§£å†³
- temp-originalquamba å¯ä»¥åŠ è½½æ—§ç‰ˆæœ¬ checkpoint (æ²¡æœ‰ --quantize_lm_head)
